{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program provided is a custom collate function, used in order to prepare a batch of sequences of data for a machine learning model. The function takes a list of sequences and pads them to the length of the longest sequence in the batch. The function then returns the padded sequences as a tensor, providing both input tensor and target tensor. The input tensor is the padded sequence with the last element removed, and the target tensor is the padded sequence with the first element removed.\n",
    "\n",
    "First, we find the longest sequence in the batch, and for each sequence in the batch, we pad it with the pad token ID <50256> to the length of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):    \n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we seperate the batch into input and target tensors. For the input tensor we remove the last token for each sequence, and for the target tensor, we shift right by 1 index for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we replace all except the first occurance of the pad token IDs in the target tensor with the ignore index <-100>. This is done to reduce certain padding tokens from being included in the training loss. (If we have a max allowed sequence length, we can also truncate the sequences to that length.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, since we are currently building a list datatype as the inputs and targets data, we convert this to tensors for pytorch and send them to the target device(which in this program is the cpu device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pepares data for the model to be trained on. The output from the given data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input Tensor --> tensor([[    0,     1,     2,     3,     4],\n",
    "                         [    5,     6, 50256, 50256, 50256],\n",
    "                         [    7,     8,     9, 50256, 50256]])\n",
    "\n",
    "                         \n",
    "Targets Tensor --> tensor([[    1,     2,     3,     4, 50256],\n",
    "                           [    6, 50256,  -100,  -100,  -100],\n",
    "                           [    8,     9, 50256,  -100,  -100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that Program C has the same output as Program A is because of the way that program C is written. Program C includes in it's *torch.tensor()* function the values of *[0,1,-100]*. \n",
    "\n",
    "In the context of tensorflow and pytorch, -100 is the ingore_index. This means that even though *logits_2* has 3 training examples, it will ignore the third training example because in the line *targets_3 = torch.tensor([0, 1, -100])*, the third value is -100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, while calculating *loss_3* as *torch.nn.functional.cross_entropy(logits_2, targets_3)*, *targets_3* only contains the first two training examples(which is identical to Program A's training examples) and therefore will result in the same loss calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
