{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Midterm Part 2**\n",
    "\n",
    "*CS 4319*\n",
    "\n",
    "*Seth Tourish*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using the best single feature:\n",
      "+---------------------+----------------+------------+\n",
      "| Model               | Best Feature   |   Accuracy |\n",
      "+=====================+================+============+\n",
      "| MLP                 | f1             |     0.6595 |\n",
      "+---------------------+----------------+------------+\n",
      "| Logistic Regression | f1             |     0.6527 |\n",
      "+---------------------+----------------+------------+\n",
      "| KNN                 | f1             |     0.6561 |\n",
      "+---------------------+----------------+------------+\n",
      "\n",
      "Results using all features:\n",
      "+---------------------+------------+\n",
      "| Model               |   Accuracy |\n",
      "+=====================+============+\n",
      "| MLP                 |     0.7161 |\n",
      "+---------------------+------------+\n",
      "| Logistic Regression |     0.7048 |\n",
      "+---------------------+------------+\n",
      "| KNN                 |     0.7002 |\n",
      "+---------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df.drop(columns=[\"class\"])\n",
    "y_train = train_df[\"class\"]\n",
    "X_test = test_df.drop(columns=[\"class\"])\n",
    "y_test = test_df[\"class\"]\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(max_iter=1000, early_stopping=True),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Grid Params\n",
    "param = {\n",
    "    \"MLP\": {\"hidden_layer_sizes\": [(2,), (5,), (10,), (15,), (20,), (50,), (50, 50), (100, 100), (200, 200), (100, 100, 100), (200, 200, 200), (500, 500, 500), (1000, 1000, 1000)]},\n",
    "    \"Logistic Regression\": {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n",
    "    \"KNN\": {\"n_neighbors\": range(1, 21)}\n",
    "}\n",
    "\n",
    "# Tune Models\n",
    "tuned_models = {}\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(model, param[model_name], cv=5)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    tuned_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "# CROSS VALIDATION FOR EACH FEATURE\n",
    "feat_score = {}\n",
    "for feature in X_train.columns:\n",
    "    X_feature = X_train[[feature]]  # Use only one feature\n",
    "    \n",
    "    scores = {}\n",
    "    for model_name, model in tuned_models.items():\n",
    "        score = np.mean(cross_val_score(model, X_feature, y_train, cv=5))\n",
    "        scores[model_name] = score\n",
    "    \n",
    "    feat_score[feature] = scores\n",
    "\n",
    "# Best Feature for Each Model\n",
    "best_features = {model: max(feat_score, key=lambda f: feat_score[f][model]) for model in tuned_models}\n",
    "\n",
    "# Use Best Feature to Train Models\n",
    "results_single_feature = {}\n",
    "for model_name, feature in best_features.items():\n",
    "    model = tuned_models[model_name]\n",
    "    model.fit(X_train[[feature]], y_train)\n",
    "    accuracy = model.score(X_test[[feature]], y_test)\n",
    "    results_single_feature[model_name] = (feature, accuracy)\n",
    "\n",
    "# TRAIN MODELS USING ALL FEATURES\n",
    "all_feature_result = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    all_feature_result\n",
    "[model_name] = accuracy\n",
    "\n",
    "# TABULATE PRINT RESULTS SECTION\n",
    "\n",
    "# Print table for best feature results\n",
    "best_single_feature_result = []\n",
    "for model, (feature, accuracy) in results_single_feature.items():\n",
    "    best_single_feature_result.append([model, feature, f\"{accuracy:.4f}\"])\n",
    "\n",
    "# Print table for best feature results\n",
    "print(\"Results using the best single feature:\")\n",
    "print(tabulate(best_single_feature_result, headers=[\"Model\", \"Best Feature\", \"Accuracy\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Print table for all feature trained model results\n",
    "all_feature_results = []\n",
    "for model, accuracy in all_feature_result.items():\n",
    "    all_feature_results.append([model, f\"{accuracy:.4f}\"])\n",
    "\n",
    "# Print table for all feature results\n",
    "print(\"\\nResults using all features:\")\n",
    "print(tabulate(all_feature_results, headers=[\"Model\", \"Accuracy\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justify Single Feature Choices**\n",
    "\n",
    "In the training of my model, my code chose feature f1 as the best feature for training the model. This was determined by using cross validation score. For the MLP best feature, f1 was chosen because it most likely contains patterns that indicate a complex correlation with the output rather than linear seperability. For the Logistic Regression, the best feature was f1 and it was chosen likely because it had the strongest linear correlation with the class variable. For the KNN best feature, f1 was chosen because it most likely provides a good seperation between the two different classes in regards to the distance based appraoch that KNN uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using the best single feature:\n",
      "+---------------------+----------------+------------+\n",
      "| Model               | Best Feature   |   Accuracy |\n",
      "+=====================+================+============+\n",
      "| MLP                 | f1             |     0.6629 |\n",
      "+---------------------+----------------+------------+\n",
      "| Logistic Regression | f1             |     0.6527 |\n",
      "+---------------------+----------------+------------+\n",
      "| KNN                 | f1             |     0.6561 |\n",
      "+---------------------+----------------+------------+\n",
      "\n",
      "Results using all features:\n",
      "+---------------------+------------+\n",
      "| Model               |   Accuracy |\n",
      "+=====================+============+\n",
      "| MLP                 |     0.7161 |\n",
      "+---------------------+------------+\n",
      "| Logistic Regression |     0.7048 |\n",
      "+---------------------+------------+\n",
      "| KNN                 |     0.7002 |\n",
      "+---------------------+------------+\n",
      "\n",
      "Best Parameters for each model:\n",
      "+---------------------+--------------------------------+\n",
      "| Model               | Best Parameters                |\n",
      "+=====================+================================+\n",
      "| MLP                 | hidden_layer_sizes: (100, 100) |\n",
      "+---------------------+--------------------------------+\n",
      "| Logistic Regression | solver: newton-cg              |\n",
      "+---------------------+--------------------------------+\n",
      "| KNN                 | n_neighbors: 20                |\n",
      "+---------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df.drop(columns=[\"class\"])\n",
    "y_train = train_df[\"class\"]\n",
    "X_test = test_df.drop(columns=[\"class\"])\n",
    "y_test = test_df[\"class\"]\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(max_iter=1000, early_stopping=True),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Grid Params\n",
    "param = {\n",
    "    \"MLP\": {\"hidden_layer_sizes\": [(2,), (5,), (10,), (15,), (20,), (50,), (50, 50), (100, 100), (200, 200), (100, 100, 100), (200, 200, 200), (500, 500, 500), (1000, 1000, 1000)]},\n",
    "    \"Logistic Regression\": {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n",
    "    \"KNN\": {\"n_neighbors\": range(1, 21)}\n",
    "}\n",
    "\n",
    "# Tune Models and store best parameters\n",
    "tuned_models = {}\n",
    "best_params = {}\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(model, param[model_name], cv=5)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    tuned_models[model_name] = grid_search.best_estimator_\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "\n",
    "# CROSS VALIDATION FOR EACH FEATURE\n",
    "feat_score = {}\n",
    "for feature in X_train.columns:\n",
    "    X_feature = X_train[[feature]]  # Use only one feature\n",
    "    \n",
    "    scores = {}\n",
    "    for model_name, model in tuned_models.items():\n",
    "        score = np.mean(cross_val_score(model, X_feature, y_train, cv=5))\n",
    "        scores[model_name] = score\n",
    "    \n",
    "    feat_score[feature] = scores\n",
    "\n",
    "# Best Feature for Each Model\n",
    "best_features = {model: max(feat_score, key=lambda f: feat_score[f][model]) for model in tuned_models}\n",
    "\n",
    "# Use Best Feature to Train Models\n",
    "results_single_feature = {}\n",
    "for model_name, feature in best_features.items():\n",
    "    model = tuned_models[model_name]\n",
    "    model.fit(X_train[[feature]], y_train)\n",
    "    accuracy = model.score(X_test[[feature]], y_test)\n",
    "    results_single_feature[model_name] = (feature, accuracy)\n",
    "\n",
    "# TRAIN MODELS USING ALL FEATURES\n",
    "all_feature_result = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    all_feature_result[model_name] = accuracy\n",
    "\n",
    "# TABULATE PRINT RESULTS SECTION\n",
    "\n",
    "# Print table for best feature results\n",
    "best_single_feature_result = []\n",
    "for model, (feature, accuracy) in results_single_feature.items():\n",
    "    best_single_feature_result.append([model, feature, f\"{accuracy:.4f}\"])\n",
    "\n",
    "# Print table for all feature trained model results\n",
    "all_feature_results = []\n",
    "for model, accuracy in all_feature_result.items():\n",
    "    all_feature_results.append([model, f\"{accuracy:.4f}\"])\n",
    "\n",
    "# Print table for best parameters\n",
    "best_param_results = []\n",
    "for model, params in best_params.items():\n",
    "    param_str = ', '.join([f\"{key}: {value}\" for key, value in params.items()])\n",
    "    best_param_results.append([model, param_str])\n",
    "\n",
    "# Print table for best single feature results\n",
    "print(\"Results using the best single feature:\")\n",
    "print(tabulate(best_single_feature_result, headers=[\"Model\", \"Best Feature\", \"Accuracy\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Print table for all feature trained model results\n",
    "print(\"\\nResults using all features:\")\n",
    "print(tabulate(all_feature_results, headers=[\"Model\", \"Accuracy\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Print table for best parameters for each model\n",
    "print(\"\\nBest Parameters for each model:\")\n",
    "print(tabulate(best_param_results, headers=[\"Model\", \"Best Parameters\"], tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
