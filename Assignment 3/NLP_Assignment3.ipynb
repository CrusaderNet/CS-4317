{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Houst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['involver', 'overling', 'lovering', 'revolving', 'involve']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Valid word check function\n",
    "def is_valid_word(word):\n",
    "    if len(word) < 7 or 'l' not in word:\n",
    "        return False\n",
    "    word_freq = FreqDist(word)\n",
    "    return all(word_freq[char] <= char_constraints[char] for char in word_freq)\n",
    "\n",
    "#Download Word Corpus and set the word set\n",
    "nltk.download('words')\n",
    "word_list = set(words.words())\n",
    "\n",
    "# Set the character fredquency constraints\n",
    "char_constraints = FreqDist({\n",
    "    'v': 2, 'e': 1, 'g': 1, 'i': 1, 'r': 1, 'o': 1, 'n': 1, 'l': 1\n",
    "})\n",
    "\n",
    "# Find valid words\n",
    "valid_words = [word for word in word_list if is_valid_word(word)]\n",
    "\n",
    "# Display the first 20 valid words\n",
    "print(valid_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled words: ['Mamata', 'Cupid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Houst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "#Spelling Error Checking Function\n",
    "def spelling_error_check(word_list, word_corpus = word_corpus):\n",
    "    misspelled_words = [word for word in word_list if word.lower() not in word_corpus]\n",
    "    return misspelled_words\n",
    "\n",
    "#Download Word Corpus and set the word set\n",
    "nltk.download('words')\n",
    "word_corpus = set(words.words())\n",
    "\n",
    "#List of words to check\n",
    "word_list = [\"Harry\", \"Mamata\", \"Awesome\", \"Love\", \"Mantra\", \"Cupid\"]\n",
    "\n",
    "#Check and display the misspelled words\n",
    "misspelled = spelling_error_check(word_list)\n",
    "print(\"Misspelled words:\", misspelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  who  what  when where   why   how \n",
      "           news   268    76   128    58     9    37 \n",
      "       religion   100    64    53    20    14    23 \n",
      "        hobbies   103    78   119    72    10    40 \n",
      "science_fiction    13    27    21    10     4    12 \n",
      "        romance    89   121   126    54    34    60 \n",
      "          humor    48    36    52    15     9    18 \n",
      "                   I  You   He  She   It   We They \n",
      "           news  179   11  191   35  115   30   62 \n",
      "       religion  155   23   69    2   73   42   18 \n",
      "        hobbies  154   51   41    8  129   59   40 \n",
      "science_fiction   98   15   52   23   29   15   14 \n",
      "        romance  951  102  366  232  144   31   69 \n",
      "          humor  239   17   49   23   48   18   28 \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
    "modals = ['who', 'what', 'when', 'where', 'why', 'how']\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "                                (genre, word)\n",
    "                                for genre in brown.categories()\n",
    "                                for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['who', 'what', 'when', 'where', 'why', 'how']\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)\n",
    "\n",
    "modals = ['I', 'You', 'He', 'She', 'It', 'We', 'They']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "                                (genre, word)\n",
    "                                for genre in brown.categories()\n",
    "                                for word in brown.words(categories=genre))\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News and Religion have the highest use of the \"who\" modal, which makes sense because news focuses on people and news events while religious texts often refer to divine figures or key historical/religious figures. \"what\" is commonly seen in Romance and Hobbies because romance texts often explore emotions and relationships, while hobby texts often describe materials and techniques. the \"when\" modal is seen frequently in Romance, News, and Hobbies texts. Romance often explore timelines of relationships, news texts often discuss timelines of events, and hobby texts often discuss schedules and timelines. The \"where\" modal is most frequently in Hobbies and News because news reports often specify location, and hobby texts often involve discussion on locations of hobbies or locations of buying supplies. The \"why\" modal is seen most frequently in Romance and Religion texts, as romance often deals with motivations and intents, while religions often discuss the reason behind faith. The \"how\" modal is most often seen in Romance and Hobbies texts, because romance texts often discuss how relationships occur or the 'how' of emotional relationships. \"how\" is frequently seen in hobby texts as hobby texts often include how to instructions.\n",
    "\n",
    "Another closed class of words that exhibit significant differences across genres is the closed class of ['I', 'You', 'He', 'She', 'It', 'We', 'They']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hyponyms* are words that are a specific instance of a broader topic. The hyponyms() function returns words that fall within the broad category of an input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hypernyms* are words that are a broader topic of a specific word. The Hypernyms() function returns words that are the more general topics of an input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part Meronyms* are words that are part of a whole concept. The part_meronyms() function returns words that make up an input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Substance Meronyms* are words that are associated with the substance a word is made up of(such as bark being a substance meronym of a tree). The substance_meronyms() function returns words that are the substances an input word is made up of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Member Holonyms* are words that represent a group to which a specific member belongs to. The member_holonyms() function returns the groups or collections that an input word is a part of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Entailment* words are words that imply a logical implication between verbs(walking entails stepping). The entailment() function returns words(verbs) that an input word(verb) entails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Synsets* are collections of words that share the same meaning. The synsets() function returns the synonym set of an input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Similarity is a concept in the Wordnet Corpus that can be used to measure how close the meanings of two words are. They do this through various functions to calculate the similarity score between two words, oftne through evaluating the hypernym paths taken to get to each word, and finding the lowest common hypernym in the hypernym hierarchy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siri has been trained using advanced voice recognition algorithms and continuous learning techniques in order to translate non-native English speakers. Using voice recognition siri takes voice input into text, and continuously learns based on historical data and NLP processing to infer word intentions based on common usage of the correctly spoken words. This continous learning allows siri to identify words that are misspoken and use the correct versions of them, allowing siri to correctly translate voice even though the speaker is a non-native english speaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorized         Brown\n",
    "\n",
    "Overlapping         (No example listed, but as per textbook, Reuters would be an example corpus)\n",
    "\n",
    "Temporal            Inaugural\n",
    "\n",
    "Isolated            Gutenberg, web text, udhr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is taking a word to is base form while still making sure it is a word in a known dictionary. Stemming is taking the affixes off of a word to reduce the word to a base form. Lemmatization is more accurate than stemming because lemmatization makes sure the word is a valid word within a known dictionary, while stemming does not. For example, given the input word \"Caring\" Stemming would remove the -ing part of the word and return just the value \"car\", while Lemmatization would reduce the word to the base word \"care\", ensuring it remains a word valid within a known dictionary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
