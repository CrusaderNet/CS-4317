{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1). What is Recurrent Neural Networks(RNN), what are the key features and what are the use cases?**\n",
    "\n",
    "Recurrent Neural Networks are a form of deep learning model that processes sequential data by maintaining a hidden state(cell state) that is updated at each time step to process a sequence. It contains a hidden state(cell state) which carries information from the previous steps. It's key features are it's short term memory, however it suffers from the vanishing gradient problem where it is hard to learn long-term dependencies. It's use cases include usually just simple sequential data tasks, however differnet forms of RNNs can have use cases such as Natural Language Processing, time-series forcasting, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2). What is Long Short-Term Memory(LSTM), what are the key features and what are the use cases?**\n",
    "\n",
    "Long Short-Term Memory is a type of Recurrent Neural Network designed to solve a key weakness of RNNs, vanishing gradients. It does this by intorducing gates to control the flow of information and therefore remember long-term dependencies. It contains it's forget gate(controls what information to forget), input gate(controls what input to store), and output gate(controls what information is passed to the next time step). The key features of the LSTM are it's long term memory. It's use cases include Natural Language Processing, time-series forcasting, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3). What is Bidirectional LSTM(BiLSTM), what are the key features and what are the use cases?**\n",
    "\n",
    "A Bidirectional LSTM is a model that utilizes a forward looking LSTM as well as a backward looking LSTM in order to capture inputs and therefore context from both the future and past information. The key features of the BiLSTM is the forward LSTM which processes information from beginning to end of the sequence, and the backward LSTM which processes information from the end to the beginning of the sequence. It's use cases include machine translation, named entity recognition, and sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
