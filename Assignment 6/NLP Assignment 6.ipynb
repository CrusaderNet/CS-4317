{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b77db68",
   "metadata": {},
   "source": [
    "**1). LR Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71295421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9770, 1024)\n",
      "(9770,)\n",
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.6370 ± 0.0211\n",
      "Mean ACC = 0.8184 ± 0.0106\n",
      "Mean SN  = 0.8182 ± 0.0173\n",
      "Mean SP  = 0.8186 ± 0.0154\n",
      "Mean PRE = 0.8187 ± 0.0124\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]      \n",
    "\n",
    "        clf = LogisticRegression(penalty='l2', solver='lbfgs', random_state=seed, max_iter=1000)\n",
    "        clf.fit(xtrain, ytrain)\n",
    "       \n",
    "        y_pred = clf.predict(xval)\n",
    "   \n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "        sn.append(sensitivity)\n",
    "        sp.append(specificity)\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "        pre.append(precision)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "    print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "    print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26912aa5",
   "metadata": {},
   "source": [
    "**2). Logistic Regression Evaluated with Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08947487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.2561 ± 0.0000\n",
      "Mean ACC = 0.8279 ± 0.0000\n",
      "Mean SN  = 0.7406 ± 0.0000\n",
      "Mean SP  = 0.8307 ± 0.0000\n",
      "Mean PRE = 0.1249 ± 0.0000\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    xtrain, ytrain = shuffle(X_train, y_train)\n",
    "\n",
    "    xval = X_test_full\n",
    "    yval = y_test_full\n",
    "\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    clf = LogisticRegression(penalty='l2', solver='lbfgs', random_state=seed, max_iter=1000)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "       \n",
    "    y_pred = clf.predict(xval)\n",
    "   \n",
    "    cm = confusion_matrix(yval, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(yval, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    cvscores.append(mcc)\n",
    "    sn.append(sensitivity)\n",
    "    sp.append(specificity)\n",
    "    acc.append(accuracy_score(yval, y_pred))\n",
    "    pre.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ce9f6",
   "metadata": {},
   "source": [
    "**1). XGBoost Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cc15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:19:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:19:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:19:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:19:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:20:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.6542 ± 0.0211\n",
      "Mean ACC = 0.8268 ± 0.0107\n",
      "Mean SN  = 0.8113 ± 0.0220\n",
      "Mean SP  = 0.8424 ± 0.0139\n",
      "Mean PRE = 0.8375 ± 0.0113\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]      \n",
    "\n",
    "        clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=seed)\n",
    "        clf.fit(xtrain, ytrain)\n",
    "       \n",
    "        y_pred = clf.predict(xval)\n",
    "   \n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "        sn.append(sensitivity)\n",
    "        sp.append(specificity)\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "        pre.append(precision)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "    print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "    print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247698b6",
   "metadata": {},
   "source": [
    "**2). XGBoost Train with Test Dataset Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4388d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:33:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.2668 ± 0.0000\n",
      "Mean ACC = 0.8463 ± 0.0000\n",
      "Mean SN  = 0.7193 ± 0.0000\n",
      "Mean SP  = 0.8504 ± 0.0000\n",
      "Mean PRE = 0.1356 ± 0.0000\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    xtrain, ytrain = shuffle(X_train, y_train)\n",
    "\n",
    "    xval = X_test_full\n",
    "    yval = y_test_full\n",
    "\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=seed)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "       \n",
    "    y_pred = clf.predict(xval)\n",
    "   \n",
    "    cm = confusion_matrix(yval, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(yval, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    cvscores.append(mcc)\n",
    "    sn.append(sensitivity)\n",
    "    sp.append(specificity)\n",
    "    acc.append(accuracy_score(yval, y_pred))\n",
    "    pre.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b912933",
   "metadata": {},
   "source": [
    "**1). RF Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e9c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.6078 ± 0.0327\n",
      "Mean ACC = 0.8013 ± 0.0163\n",
      "Mean SN  = 0.7371 ± 0.0224\n",
      "Mean SP  = 0.8655 ± 0.0177\n",
      "Mean PRE = 0.8458 ± 0.0193\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]      \n",
    "\n",
    "        clf = RandomForestClassifier(random_state=seed)\n",
    "        clf.fit(xtrain, ytrain)\n",
    "       \n",
    "        y_pred = clf.predict(xval)\n",
    "   \n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "        sn.append(sensitivity)\n",
    "        sp.append(specificity)\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "        pre.append(precision)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "    print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "    print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c7af4",
   "metadata": {},
   "source": [
    "**2). RF Train with Test Dataset Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcca9e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.2589 ± 0.0000\n",
      "Mean ACC = 0.8663 ± 0.0000\n",
      "Mean SN  = 0.6444 ± 0.0000\n",
      "Mean SP  = 0.8735 ± 0.0000\n",
      "Mean PRE = 0.1425 ± 0.0000\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    xtrain, ytrain = shuffle(X_train, y_train)\n",
    "\n",
    "    xval = X_test_full\n",
    "    yval = y_test_full\n",
    "\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=seed)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "       \n",
    "    y_pred = clf.predict(xval)\n",
    "   \n",
    "    cm = confusion_matrix(yval, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(yval, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    cvscores.append(mcc)\n",
    "    sn.append(sensitivity)\n",
    "    sp.append(specificity)\n",
    "    acc.append(accuracy_score(yval, y_pred))\n",
    "    pre.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c616f3",
   "metadata": {},
   "source": [
    "**1). SVM Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.6725 ± 0.0181\n",
      "Mean ACC = 0.8359 ± 0.0091\n",
      "Mean SN  = 0.8164 ± 0.0133\n",
      "Mean SP  = 0.8555 ± 0.0117\n",
      "Mean PRE = 0.8497 ± 0.0107\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]      \n",
    "\n",
    "        clf = SVC(random_state=seed)\n",
    "        clf.fit(xtrain, ytrain)\n",
    "       \n",
    "        y_pred = clf.predict(xval)\n",
    "   \n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "        sn.append(sensitivity)\n",
    "        sp.append(specificity)\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "        pre.append(precision)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "    print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "    print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a0ae9",
   "metadata": {},
   "source": [
    "**2). SVM Train with Test Dataset Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abba760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.3089 ± 0.0000\n",
      "Mean ACC = 0.8750 ± 0.0000\n",
      "Mean SN  = 0.7326 ± 0.0000\n",
      "Mean SP  = 0.8796 ± 0.0000\n",
      "Mean PRE = 0.1657 ± 0.0000\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    xtrain, ytrain = shuffle(X_train, y_train)\n",
    "\n",
    "    xval = X_test_full\n",
    "    yval = y_test_full\n",
    "\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    clf = SVC(random_state=seed)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "       \n",
    "    y_pred = clf.predict(xval)\n",
    "   \n",
    "    cm = confusion_matrix(yval, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(yval, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    cvscores.append(mcc)\n",
    "    sn.append(sensitivity)\n",
    "    sp.append(specificity)\n",
    "    acc.append(accuracy_score(yval, y_pred))\n",
    "    pre.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d3d3b",
   "metadata": {},
   "source": [
    "**1). MLP Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb49b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  64916\n",
      "(9770, 1024)\n",
      "(9770,)\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.5899 ± 0.0164\n",
      "Mean ACC = 0.7941 ± 0.0080\n",
      "Mean SN  = 0.7578 ± 0.0138\n",
      "Mean SP  = 0.8303 ± 0.0163\n",
      "Mean PRE = 0.8173 ± 0.0137\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt', header=None)\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt', header=None)\n",
    "\n",
    "Header_name = [\"Position\", \"PID\", \"Position_redundant\", \"81 Window sequence\", \"S or T\"] + [i for i in range(1, 1025)]\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "O_linked_training = pd.concat([df_positive, df_negative], ignore_index=True)\n",
    "df_Train_array = np.array(O_linked_training.drop([\"Position\", \"PID\", \"Position_redundant\", \"81 Window sequence\", \"S or T\"], axis=1))\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "y_train_full = np.array([1]*4885 + [0]*114144)\n",
    "\n",
    "a = random.sample(range(1, 1000000), 1)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    print(\"Seed : \", seed)\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        Y_train_10 = tf.keras.utils.to_categorical(ytrain, 2)\n",
    "\n",
    "        model = Sequential([\n",
    "            Input(shape=(1024,)),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu', name=\"Dense_1\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(2, activation='softmax', name=\"Dense_2\")\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                      loss=\"binary_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        model.fit(xtrain, Y_train_10, epochs=35, verbose=0, batch_size=256)\n",
    "\n",
    "        Y_pred = model.predict(xval)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "        sn.append(sensitivity)\n",
    "        sp.append(specificity)\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "        pre.append(precision)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "    print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "    print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "    print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b88e1a",
   "metadata": {},
   "source": [
    "**2). MLP Train with Test Dataset Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35e9e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  388561\n",
      "Epoch 1/400\n",
      "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6956 - loss: 0.5881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6987 - loss: 0.5847 - val_accuracy: 0.7861 - val_loss: 0.4672 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m24/35\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8234 - loss: 0.4329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8257 - loss: 0.4275 - val_accuracy: 0.8014 - val_loss: 0.4298 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m31/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8579 - loss: 0.3572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8576 - loss: 0.3581 - val_accuracy: 0.8199 - val_loss: 0.4184 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8802 - loss: 0.3114 - val_accuracy: 0.8025 - val_loss: 0.4463 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8990 - loss: 0.2603 - val_accuracy: 0.8168 - val_loss: 0.4717 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9128 - loss: 0.2397 - val_accuracy: 0.8168 - val_loss: 0.4955 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9356 - loss: 0.1924 - val_accuracy: 0.8137 - val_loss: 0.5430 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m24/35\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9439 - loss: 0.1561\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9428 - loss: 0.1574 - val_accuracy: 0.8168 - val_loss: 0.6154 - learning_rate: 0.0010\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Seed :  733509\n",
      "Epoch 1/400\n",
      "\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6967 - loss: 0.5851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7017 - loss: 0.5800 - val_accuracy: 0.7871 - val_loss: 0.4600 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8288 - loss: 0.4139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8288 - loss: 0.4139 - val_accuracy: 0.7963 - val_loss: 0.4357 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8505 - loss: 0.3718 - val_accuracy: 0.7943 - val_loss: 0.4598 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8685 - loss: 0.3297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8687 - loss: 0.3293 - val_accuracy: 0.8066 - val_loss: 0.4456 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m32/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8990 - loss: 0.2728"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8986 - loss: 0.2732 - val_accuracy: 0.8096 - val_loss: 0.4771 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m24/35\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9214 - loss: 0.2229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9197 - loss: 0.2244 - val_accuracy: 0.8127 - val_loss: 0.4686 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9297 - loss: 0.1960 - val_accuracy: 0.8004 - val_loss: 0.5399 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.1627 - val_accuracy: 0.8096 - val_loss: 0.6261 - learning_rate: 0.0010\n",
      "Epoch 9/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9536 - loss: 0.1320 - val_accuracy: 0.8055 - val_loss: 0.6878 - learning_rate: 0.0010\n",
      "Epoch 10/400\n",
      "\u001b[1m32/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9575 - loss: 0.1199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9572 - loss: 0.1199 - val_accuracy: 0.8158 - val_loss: 0.7241 - learning_rate: 0.0010\n",
      "Epoch 11/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9577 - loss: 0.1126 - val_accuracy: 0.8096 - val_loss: 0.7565 - learning_rate: 0.0010\n",
      "Epoch 12/400\n",
      "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.0965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9663 - loss: 0.0968 - val_accuracy: 0.8270 - val_loss: 0.7019 - learning_rate: 0.0010\n",
      "Epoch 13/400\n",
      "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9701 - loss: 0.0851 - val_accuracy: 0.8291 - val_loss: 0.7525 - learning_rate: 0.0010\n",
      "Epoch 14/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9756 - loss: 0.0750 - val_accuracy: 0.8035 - val_loss: 0.8512 - learning_rate: 0.0010\n",
      "Epoch 15/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.0692 - val_accuracy: 0.8045 - val_loss: 0.9176 - learning_rate: 0.0010\n",
      "Epoch 16/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9704 - loss: 0.0822 - val_accuracy: 0.8045 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 17/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9750 - loss: 0.0736 - val_accuracy: 0.8188 - val_loss: 0.9234 - learning_rate: 0.0010\n",
      "Epoch 18/400\n",
      "\u001b[1m24/35\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9830 - loss: 0.0527\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9814 - loss: 0.0553 - val_accuracy: 0.7994 - val_loss: 0.9763 - learning_rate: 0.0010\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Seed :  77778\n",
      "Epoch 1/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6957 - loss: 0.5909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6973 - loss: 0.5894 - val_accuracy: 0.7789 - val_loss: 0.4749 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m24/35\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8267 - loss: 0.4228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8281 - loss: 0.4192 - val_accuracy: 0.7922 - val_loss: 0.4630 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8539 - loss: 0.3682"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8540 - loss: 0.3680 - val_accuracy: 0.8014 - val_loss: 0.4610 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8797 - loss: 0.3272 - val_accuracy: 0.7973 - val_loss: 0.4691 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9066 - loss: 0.2630 - val_accuracy: 0.8014 - val_loss: 0.4919 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9194 - loss: 0.2307 - val_accuracy: 0.7932 - val_loss: 0.5506 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9309 - loss: 0.1991 - val_accuracy: 0.8014 - val_loss: 0.6604 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m32/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9387 - loss: 0.1647\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9387 - loss: 0.1652 - val_accuracy: 0.8004 - val_loss: 0.6697 - learning_rate: 0.0010\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "=== Final Results over 3 runs ===\n",
      "Mean MCC = 0.5634 ± 0.0204\n",
      "Mean ACC = 0.7772 ± 0.0101\n",
      "Mean SN  = 0.6898 ± 0.0208\n",
      "Mean SP  = 0.8645 ± 0.0196\n",
      "Mean PRE = 0.8364 ± 0.0176\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "a = random.sample(range(1, 1000000), 10)\n",
    "\n",
    "# Training Dataset\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "# Training Starts From Here\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Run the model for Three times and choose the best answer among them\n",
    "a = random.sample(range(1, 1000000), 3)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Lists to collect metrics across 3 runs\n",
    "mcc_list, acc_list, sn_list, sp_list, pre_list = [], [], [], [], []\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    print(\"Seed : \", seed)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "    x_train, x_val, y_train_1, y_val = train_test_split(X_train, y_train, random_state=21, test_size=0.1)\n",
    "\n",
    "    y_train_1 = tf.keras.utils.to_categorical(y_train_1, 2)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1024,)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu', name=\"Dense_1\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax', name=\"Dense_2\"))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"ROC_ROC_Premise_Assumption.h5\", \n",
    "                                                       monitor='val_accuracy',\n",
    "                                                       verbose=0, \n",
    "                                                       save_weights_only=False,\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "    reduce_lr_acc = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.001, patience=5, verbose=1, min_delta=1e-4, mode='max')\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max')\n",
    "\n",
    "    history = model.fit(x_train, y_train_1, epochs=400, verbose=1, batch_size=256,\n",
    "                        callbacks=[checkpointer, reduce_lr_acc, early_stopping], validation_data=(x_val, y_val))\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_independent, y_independent = rus.fit_resample(X_test_full, y_test_full)\n",
    "\n",
    "    Y_pred = model.predict(X_independent)\n",
    "    Y_pred = (Y_pred > 0.5)\n",
    "    y_pred = [np.argmax(y) for y in Y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_independent, y_pred)\n",
    "\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(y_independent, y_pred)\n",
    "    acc = accuracy_score(y_independent, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = precision_score(y_independent, y_pred, zero_division=0)\n",
    "\n",
    "    # Append to lists\n",
    "    mcc_list.append(mcc)\n",
    "    acc_list.append(acc)\n",
    "    sn_list.append(sensitivity)\n",
    "    sp_list.append(specificity)\n",
    "    pre_list.append(precision)\n",
    "\n",
    "# === After all 3 runs, print the average and std ===\n",
    "print(\"\\n=== Final Results over 3 runs ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(mcc_list), np.std(mcc_list)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc_list), np.std(acc_list)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn_list), np.std(sn_list)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp_list), np.std(sp_list)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre_list), np.std(pre_list)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7e1a1",
   "metadata": {},
   "source": [
    "**1). 1DCNN Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f2f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9770, 1024, 1)\n",
      "(9770,)\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "=== Final Results ===\n",
      "Mean MCC = 0.6290 ± 0.0146\n",
      "Mean ACC = 0.8139 ± 0.0072\n",
      "Mean SN  = 0.7879 ± 0.0158\n",
      "Mean SP  = 0.8399 ± 0.0174\n",
      "Mean PRE = 0.8315 ± 0.0141\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, roc_curve, auc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt', header=None)\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt', header=None)\n",
    "\n",
    "Header_name = [\"Position\", \"PID\", \"Position_redundant\", \"81 Window sequence\", \"S or T\"]\n",
    "col_of_feature = [i for i in range(1, 1025)]\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "O_linked_training = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\", \"PID\", \"Position_redundant\", \"81 Window sequence\", \"S or T\"], axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "y_train_full = np.array([1] * 4885 + [0] * 114144)\n",
    "\n",
    "seed = 42\n",
    "rus = RandomUnderSampler(random_state = seed)\n",
    "X_train_full, y_train_full = rus.fit_resample(X_train_full,y_train_full)\n",
    "\n",
    "X_train_full = X_train_full.reshape(X_train_full.shape[0], 1024, 1)\n",
    "X_train_full, y_train_full = shuffle(X_train_full, y_train_full)\n",
    "\n",
    "print(X_train_full.shape)\n",
    "print(y_train_full.shape)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "cvscores, sn, sp, acc, pre = list(), list(), list(), list(), list()\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train_full, y_train_full):\n",
    "    xtrain, xval = X_train_full[train_index], X_train_full[test_index]\n",
    "    ytrain, yval = y_train_full[train_index], y_train_full[test_index]\n",
    "\n",
    "    X_train_10 = xtrain\n",
    "    Y_train_10 = ytrain\n",
    "    Y_train_10 = tf.keras.utils.to_categorical(Y_train_10, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1024, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train_10, Y_train_10, epochs=35, verbose=0, batch_size=256)\n",
    "\n",
    "    Y_pred = model.predict(xval)\n",
    "    Y_pred = (Y_pred > 0.5)\n",
    "    y_pred = [np.argmax(y) for y in Y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(yval, y_pred)\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(yval, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    acc.append(accuracy_score(yval, y_pred))\n",
    "    cvscores.append(mcc)\n",
    "    sn.append(sensitivity)\n",
    "    sp.append(specificity)\n",
    "    pre.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc), np.std(acc)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn), np.std(sn)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp), np.std(sp)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre), np.std(pre)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9717fcb",
   "metadata": {},
   "source": [
    "**2). 1DCNN Train with Test Dataset Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c7c4ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  83230\n",
      "Epoch 1/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.6582 - loss: 0.6149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.6598 - loss: 0.6136 - val_accuracy: 0.7789 - val_loss: 0.4706 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7936 - loss: 0.4799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.7937 - loss: 0.4797 - val_accuracy: 0.8199 - val_loss: 0.4318 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.8234 - loss: 0.4310 - val_accuracy: 0.8035 - val_loss: 0.4196 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.8360 - loss: 0.3984 - val_accuracy: 0.8178 - val_loss: 0.3951 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8410 - loss: 0.3809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.8412 - loss: 0.3807 - val_accuracy: 0.8209 - val_loss: 0.4148 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8565 - loss: 0.3531 - val_accuracy: 0.8137 - val_loss: 0.3954 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.8726 - loss: 0.3300 - val_accuracy: 0.8188 - val_loss: 0.4008 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.8766 - loss: 0.3139 - val_accuracy: 0.8096 - val_loss: 0.4360 - learning_rate: 0.0010\n",
      "Epoch 9/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8680 - loss: 0.3123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.8682 - loss: 0.3122 - val_accuracy: 0.8219 - val_loss: 0.4189 - learning_rate: 0.0010\n",
      "Epoch 10/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8905 - loss: 0.2664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.8905 - loss: 0.2667 - val_accuracy: 0.8250 - val_loss: 0.4173 - learning_rate: 0.0010\n",
      "Epoch 11/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.9000 - loss: 0.2511 - val_accuracy: 0.8188 - val_loss: 0.4260 - learning_rate: 0.0010\n",
      "Epoch 12/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9068 - loss: 0.2441 - val_accuracy: 0.8229 - val_loss: 0.4295 - learning_rate: 0.0010\n",
      "Epoch 13/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9143 - loss: 0.2210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.9142 - loss: 0.2212 - val_accuracy: 0.8280 - val_loss: 0.4620 - learning_rate: 0.0010\n",
      "Epoch 14/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9152 - loss: 0.2148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.9153 - loss: 0.2148 - val_accuracy: 0.8321 - val_loss: 0.4703 - learning_rate: 0.0010\n",
      "Epoch 15/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.9325 - loss: 0.1807 - val_accuracy: 0.8291 - val_loss: 0.4692 - learning_rate: 0.0010\n",
      "Epoch 16/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.9309 - loss: 0.1764 - val_accuracy: 0.8127 - val_loss: 0.5253 - learning_rate: 0.0010\n",
      "Epoch 17/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9406 - loss: 0.1565 - val_accuracy: 0.8240 - val_loss: 0.5125 - learning_rate: 0.0010\n",
      "Epoch 18/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9491 - loss: 0.1484 - val_accuracy: 0.8045 - val_loss: 0.5727 - learning_rate: 0.0010\n",
      "Epoch 19/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9396 - loss: 0.1526\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9396 - loss: 0.1525 - val_accuracy: 0.8117 - val_loss: 0.5954 - learning_rate: 0.0010\n",
      "\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Seed :  858091\n",
      "Epoch 1/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.6463 - loss: 0.6244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.6480 - loss: 0.6230 - val_accuracy: 0.7779 - val_loss: 0.4727 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7821 - loss: 0.4859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.7824 - loss: 0.4855 - val_accuracy: 0.8117 - val_loss: 0.4188 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8175 - loss: 0.4330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.8176 - loss: 0.4328 - val_accuracy: 0.8280 - val_loss: 0.3900 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8335 - loss: 0.4027 - val_accuracy: 0.8250 - val_loss: 0.3913 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8460 - loss: 0.3732 - val_accuracy: 0.8260 - val_loss: 0.3752 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.8629 - loss: 0.3370 - val_accuracy: 0.8178 - val_loss: 0.3904 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.8656 - loss: 0.3305 - val_accuracy: 0.8229 - val_loss: 0.3986 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8718 - loss: 0.3151\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8718 - loss: 0.3151 - val_accuracy: 0.8260 - val_loss: 0.4031 - learning_rate: 0.0010\n",
      "\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Seed :  983987\n",
      "Epoch 1/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6656 - loss: 0.6152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.6671 - loss: 0.6139 - val_accuracy: 0.7769 - val_loss: 0.4737 - learning_rate: 0.0010\n",
      "Epoch 2/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8005 - loss: 0.4721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.8005 - loss: 0.4718 - val_accuracy: 0.7984 - val_loss: 0.4279 - learning_rate: 0.0010\n",
      "Epoch 3/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8348 - loss: 0.4107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.8346 - loss: 0.4108 - val_accuracy: 0.8055 - val_loss: 0.4223 - learning_rate: 0.0010\n",
      "Epoch 4/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8463 - loss: 0.3847 - val_accuracy: 0.7902 - val_loss: 0.4721 - learning_rate: 0.0010\n",
      "Epoch 5/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8413 - loss: 0.3731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.8415 - loss: 0.3729 - val_accuracy: 0.8137 - val_loss: 0.3957 - learning_rate: 0.0010\n",
      "Epoch 6/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8601 - loss: 0.3354 - val_accuracy: 0.8127 - val_loss: 0.4190 - learning_rate: 0.0010\n",
      "Epoch 7/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8734 - loss: 0.3110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.8733 - loss: 0.3112 - val_accuracy: 0.8178 - val_loss: 0.4045 - learning_rate: 0.0010\n",
      "Epoch 8/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.8760 - loss: 0.3044 - val_accuracy: 0.8086 - val_loss: 0.4306 - learning_rate: 0.0010\n",
      "Epoch 9/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.8818 - loss: 0.2867 - val_accuracy: 0.8076 - val_loss: 0.4140 - learning_rate: 0.0010\n",
      "Epoch 10/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8952 - loss: 0.2616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - accuracy: 0.8951 - loss: 0.2619 - val_accuracy: 0.8188 - val_loss: 0.4248 - learning_rate: 0.0010\n",
      "Epoch 11/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9026 - loss: 0.2464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.9025 - loss: 0.2464 - val_accuracy: 0.8229 - val_loss: 0.4430 - learning_rate: 0.0010\n",
      "Epoch 12/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9165 - loss: 0.2167 - val_accuracy: 0.8209 - val_loss: 0.4756 - learning_rate: 0.0010\n",
      "Epoch 13/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9186 - loss: 0.2053 - val_accuracy: 0.8127 - val_loss: 0.4859 - learning_rate: 0.0010\n",
      "Epoch 14/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9251 - loss: 0.1957 - val_accuracy: 0.8168 - val_loss: 0.5129 - learning_rate: 0.0010\n",
      "Epoch 15/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9315 - loss: 0.1858 - val_accuracy: 0.8199 - val_loss: 0.4758 - learning_rate: 0.0010\n",
      "Epoch 16/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9405 - loss: 0.1624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.9405 - loss: 0.1624 - val_accuracy: 0.8301 - val_loss: 0.5429 - learning_rate: 0.0010\n",
      "Epoch 17/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9473 - loss: 0.1393 - val_accuracy: 0.8199 - val_loss: 0.5373 - learning_rate: 0.0010\n",
      "Epoch 18/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9570 - loss: 0.1244 - val_accuracy: 0.8127 - val_loss: 0.5394 - learning_rate: 0.0010\n",
      "Epoch 19/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9553 - loss: 0.1248 - val_accuracy: 0.8004 - val_loss: 0.5426 - learning_rate: 0.0010\n",
      "Epoch 20/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9577 - loss: 0.1232 - val_accuracy: 0.8127 - val_loss: 0.5669 - learning_rate: 0.0010\n",
      "Epoch 21/400\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9534 - loss: 0.1213\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9536 - loss: 0.1211 - val_accuracy: 0.8199 - val_loss: 0.6220 - learning_rate: 0.0010\n",
      "\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "=== Final Results over 3 runs ===\n",
      "Mean MCC = 0.2446 ± 0.0326\n",
      "Mean ACC = 0.8276 ± 0.0360\n",
      "Mean SN  = 0.7014 ± 0.0155\n",
      "Mean SP  = 0.8317 ± 0.0376\n",
      "Mean PRE = 0.1247 ± 0.0253\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "seed = 42\n",
    "rus = RandomUnderSampler(random_state = seed)\n",
    "X_train_full, y_train_full = rus.fit_resample(X_train_full,y_train_full)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "a = random.sample(range(1, 1000000), 3)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "mcc_list, acc_list, sn_list, sp_list, pre_list = [], [], [], [], []\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    print(\"Seed : \", seed)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full, y_train_full)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 1024, 1)\n",
    "\n",
    "    x_train, x_val, y_train_1, y_val = train_test_split(X_train, y_train, random_state=21, test_size=0.1)\n",
    "\n",
    "    y_train_1 = tf.keras.utils.to_categorical(y_train_1, 2)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1024, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', name='Conv_1D_1_add'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu', name=\"Dense_1\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax', name=\"Dense_2\"))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"ROC_ROC_Premise_Assumption.h5\", \n",
    "                                                       monitor='val_accuracy',\n",
    "                                                       verbose=0, \n",
    "                                                       save_weights_only=False,\n",
    "                                                       save_best_only=True)\n",
    "    reduce_lr_acc = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.001, patience=5, verbose=1, min_delta=1e-4, mode='max')\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max')\n",
    "\n",
    "    history = model.fit(x_train, y_train_1, epochs=400, verbose=1, batch_size=256,\n",
    "                        callbacks=[checkpointer, reduce_lr_acc, early_stopping], validation_data=(x_val, y_val))\n",
    "    \n",
    "    X_independent, y_independent = X_test_full, y_test_full\n",
    "    X_independent = X_independent.reshape(X_independent.shape[0], 1024, 1)\n",
    "\n",
    "    Y_pred = model.predict(X_independent)\n",
    "    Y_pred = (Y_pred > 0.5)\n",
    "    y_pred = [np.argmax(y) for y in Y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_independent, y_pred)\n",
    "\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(y_independent, y_pred)\n",
    "    acc = accuracy_score(y_independent, y_pred)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = precision_score(y_independent, y_pred, zero_division=0)\n",
    "\n",
    "    mcc_list.append(mcc)\n",
    "    acc_list.append(acc)\n",
    "    sn_list.append(sensitivity)\n",
    "    sp_list.append(specificity)\n",
    "    pre_list.append(precision)\n",
    "\n",
    "print(\"\\n=== Final Results over 3 runs ===\")\n",
    "print(\"Mean MCC = %.4f ± %.4f\" % (np.mean(mcc_list), np.std(mcc_list)))\n",
    "print(\"Mean ACC = %.4f ± %.4f\" % (np.mean(acc_list), np.std(acc_list)))\n",
    "print(\"Mean SN  = %.4f ± %.4f\" % (np.mean(sn_list), np.std(sn_list)))\n",
    "print(\"Mean SP  = %.4f ± %.4f\" % (np.mean(sp_list), np.std(sp_list)))\n",
    "print(\"Mean PRE = %.4f ± %.4f\" % (np.mean(pre_list), np.std(pre_list)))\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1098e",
   "metadata": {},
   "source": [
    "**3). Twitter Sentiment Analysis using Keras Embedding with 1DCCNN, BiLSTM, and LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccf8170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Houst\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_38\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_38\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_2          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_13 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_104 (\u001b[38;5;33mDropout\u001b[0m)           │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_2          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_105 (\u001b[38;5;33mDropout\u001b[0m)           │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4942 - loss: 0.6959 - val_accuracy: 0.5000 - val_loss: 0.6919\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5728 - loss: 0.6767 - val_accuracy: 0.6650 - val_loss: 0.6205\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7954 - loss: 0.4791 - val_accuracy: 0.7200 - val_loss: 0.5533\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9325 - loss: 0.2215 - val_accuracy: 0.7225 - val_loss: 0.6554\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9770 - loss: 0.1066 - val_accuracy: 0.6825 - val_loss: 0.8148\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_39\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_39\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_106 (\u001b[38;5;33mDropout\u001b[0m)           │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_88 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_89 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.4855 - loss: 0.6969 - val_accuracy: 0.5000 - val_loss: 0.6938\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.5052 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6935\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.4886 - loss: 0.6941 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.5127 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6939\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.4940 - loss: 0.6944 - val_accuracy: 0.5000 - val_loss: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_40\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_40\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_107 (\u001b[38;5;33mDropout\u001b[0m)           │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_90 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_91 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.5388 - loss: 0.6855 - val_accuracy: 0.7175 - val_loss: 0.5884\n",
      "Epoch 2/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.8134 - loss: 0.4313 - val_accuracy: 0.7362 - val_loss: 0.5738\n",
      "Epoch 3/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9504 - loss: 0.1505 - val_accuracy: 0.7038 - val_loss: 0.7873\n",
      "Epoch 4/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.9853 - loss: 0.0495 - val_accuracy: 0.7312 - val_loss: 0.9233\n",
      "Epoch 5/5\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - accuracy: 0.9924 - loss: 0.0212 - val_accuracy: 0.6975 - val_loss: 1.5654\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Model: 1D CNN | Sentence: \"worst services. will not come again\" | Prediction: 0 (Negative)\n",
      "Model: 1D CNN | Sentence: \"thank you for watching\" | Prediction: 1 (Positive)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Model: LSTM | Sentence: \"worst services. will not come again\" | Prediction: 0 (Negative)\n",
      "Model: LSTM | Sentence: \"thank you for watching\" | Prediction: 0 (Negative)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Model: BiLSTM | Sentence: \"worst services. will not come again\" | Prediction: 0 (Negative)\n",
      "Model: BiLSTM | Sentence: \"thank you for watching\" | Prediction: 1 (Positive)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('twitter4000.csv')\n",
    "\n",
    "text = df['twitts'].tolist()\n",
    "\n",
    "y = df['sentiment']\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)\n",
    "\n",
    "vocab_size = len(token.word_index) + 1\n",
    "\n",
    "encoded_text = token.texts_to_sequences(text)\n",
    "\n",
    "max_length = 120\n",
    "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2, stratify = y)\n",
    "\n",
    "vec_size = 300\n",
    "\n",
    "# 1DCNN Model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(vocab_size, vec_size, input_length=max_length))\n",
    "cnn_model.add(Conv1D(64, 8, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(32, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(16, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(vocab_size, vec_size, input_length=max_length))\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "# BiLSTM Model\n",
    "bilstm_model = Sequential()\n",
    "bilstm_model.add(Embedding(vocab_size, vec_size, input_length=max_length))\n",
    "bilstm_model.add(Bidirectional(LSTM(128)))\n",
    "bilstm_model.add(Dropout(0.5))\n",
    "bilstm_model.add(Dense(32, activation='relu'))\n",
    "bilstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "bilstm_model.summary()\n",
    "\n",
    "bilstm_model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "def get_encoded(x):\n",
    "    x = token.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen=max_length, padding='post')\n",
    "    return x\n",
    "\n",
    "# Sentences you want to test\n",
    "sentences = ['worst services. will not come again', 'thank you for watching']\n",
    "\n",
    "# Encode sentences once (batch them)\n",
    "X_sentences = get_encoded(sentences)\n",
    "\n",
    "# Models to use\n",
    "models = {\n",
    "    '1D CNN': cnn_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'BiLSTM': bilstm_model\n",
    "}\n",
    "\n",
    "# Predict and display\n",
    "for model_name, model in models.items():\n",
    "    preds = model.predict(X_sentences)  # Batch predict all sentences at once\n",
    "    preds = (preds > 0.5).astype(\"int32\").flatten()  # Convert predictions to 0/1\n",
    "\n",
    "    for sentence, pred in zip(sentences, preds):\n",
    "        sentiment = 'Positive' if pred == 1 else 'Negative'\n",
    "        print(f\"Model: {model_name} | Sentence: \\\"{sentence}\\\" | Prediction: {pred} ({sentiment})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664bc97",
   "metadata": {},
   "source": [
    "**4). What is a Bag of N-grams? How is it better than Bag of Words? Does it have some context information? Can you explain it with a program? List the Pros and cons of a Bag of N-grams.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa71678",
   "metadata": {},
   "source": [
    "Bag of N-grams is an extension of the concept of Bag of Words, where instead of using just one word without a sequence, you use sequences of multiple words (N words per sequence) inside the bag. This allows us to get some idea of sequencing. This is better than Bag of Words because it includes some context information, as it does not ingore all the relationships between words like Bag of Words does. \n",
    "\n",
    "Pros:\n",
    "- Captures order of words\n",
    "- Improves accuracy over BoW\n",
    "Cons:\n",
    "- High dimensionality\n",
    "- Lots of 0s since many n-grams can not be present in different sentences\n",
    "- Can lead to overfitting with small amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd66baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['be hard' 'can be' 'field of' 'is field' 'is fun' 'language processing'\n",
      " 'learning is' 'love natural' 'love the' 'machine learning'\n",
      " 'natural language' 'of machine' 'processing can' 'processing is'\n",
      " 'the outdoors']\n",
      "[[0 0 0 0 0 1 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 1 1 0 1 0 0 0 1 1 1 0 1 0]\n",
      " [0 0 0 0 1 0 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example sentences\n",
    "corpus = [\n",
    "    \"I love Natural Language Processing\",\n",
    "    \"I love the outdoors\",\n",
    "    \"Natural Language Processing can be hard\",\n",
    "    \"Natural Language Processing is a field of Machine Learning\",\n",
    "    \"Machine Learning is fun\"\n",
    "]\n",
    "\n",
    "# Create a bigram (n-gram) vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Feature names (bigrams)\n",
    "print(\"Bigrams:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show the feature matrix\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ca3bf",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Here, we have 5 Sentences. In the code, we create segments of two words for each sentence, and that gives us the list of bigrams in our corpus(the output line labeled Bigrams). Then, we output the matrix where each row is one of the sentences, and each column represents one bigram from out list of bigrams. For example, in Sentence 3, both bigrams \"be hard\" and \"can be\" are present (Columns 1 and 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89daa2a7",
   "metadata": {},
   "source": [
    "**5). Explain Part-of-Speech (POS) tagging using the Hidden Markov Model (HMM). Why is the Viterbi Algorithm used to optimize the Hidden Markov Model? What do emission and transition probabilities represent in this context? Provide an explanation of HMM with a sample program.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adeb59",
   "metadata": {},
   "source": [
    "Part-of-Speech tagging is assigning a label to each word in a sentence, or a part-of-speech label (Det, Noun, Verb, Adverb, etc.). The Hidden Markov Model is a method of POS tagging utilizing probabilities or states, using things called Emission and Transition probabilities. The Transition probabilities are the probability of moving from one Part-of-Speech tag to another, such as the probability of moving from Noun to Verb. The Emission Probabilities are the probability of a word being part of a Part-of-Speech tag, such as the probability of \"lizard\" being a Noun. Viterbi alogrithm is used to optimize the Hidden Markov Model because it only continues on the most probable changes in the state (or the most probable tag given the previous tag). Doing so allows it to not need to look at the whole sentence, but just the previous tag into the current tag it is trying to predict. Emission probability in this context represents the probabilities of a word being each type of POS tag, while Transition probability represents the probability of a current tag given the previous tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41319fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Houst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Houst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRON'), ('runs', 'VERB'), ('fast', 'ADV')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "nltk.download('treebank')\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "\n",
    "train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "\n",
    "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
    "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
    "\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "#     print(tag_list)\n",
    "    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    #now calculate the total number of times the passed word occurred as the passed tag.\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    " \n",
    "     \n",
    "    return (count_w_given_tag, count_tag)\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n",
    "test_sent=\"She runs fast\"\n",
    "pred_tags_withoutRules= Viterbi(test_sent.split())\n",
    "print(pred_tags_withoutRules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8567f8",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "First we import datasets to act as out trained words for our imported list of tags. \n",
    "\n",
    "We then have a function to help figure out Emission probabilities. It takes in a word, a tag, and our trained list of tagged words. It gets the number of times the tag appears in the trained bag, and also the number of times the passed word has that tag. It then returns both of those.\n",
    "\n",
    "We then have another function that helps figure out the Transition probabilities. It takes in two seperate tags and the list of tagged words, and counts how often that the second tag appears after the first tag.\n",
    "\n",
    "We then build the Emission probability matrix for probabilities of each tag to another tag. \n",
    "\n",
    "Then we use the viterbi algorithm (explained using an example sentence):\n",
    "\n",
    "First, we split the sentence. It is split into ['She', 'runs', 'fast']. In Viterbi function, we have a state array that starts empty, and holds the states of each tag as they are predicted. We have another list of tags that we extract from the training bag. \n",
    "\n",
    "For each word in the sentence 'She runs fast', we do the following:\n",
    "\n",
    "- First, we create a list that stores the state probabilities for each tag at the current word.\n",
    "- We have a loop that loops through each tag in the tagged words list, and for each tag:\n",
    "    - If this is the first word in the sentence (aka 'She'), we get the transition probabilities for the start symbol to each tag for the first word. This list is shown in the first transition probability screenshot provided.\n",
    "    - If this is not the first word in the sentence, we get the transition probabilities from the last state to each tag in the tags list. This list is shown as the other two transition probability list screenshots provided.\n",
    "    - For each tag, we compute the emission probability(or the probability of the word appearing given a certain tag).\n",
    "    - For each tag, we then compute the state probabilities, which is the emission probability multiplied by the transition probability.\n",
    "    - We then append this probability to the probability list we created at the beginning of the first for loop.\n",
    "\n",
    "- After we fully run this inside for loop, we find the maximum state probability from the state probability list. This will be what the function tags the current word as.\n",
    "- We then append this highest probability tag to the state list.\n",
    "\n",
    "After all of this, we return a completed list that contains each original input word and it's tag that the Viterbi function came up with.\n",
    "\n",
    "This viterbi algorithm returns a list of each input word from a sentence and its predicted tag based off of its emission, transition, and state probabilities.\n",
    "\n",
    "This is how the HMM and Viterbi is used in POS tagging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
