{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6517f9",
   "metadata": {},
   "source": [
    "# 10 Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142746aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import imblearn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a9bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/t326h379/OGP\")\n",
    "\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d2ef07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>PID</th>\n",
       "      <th>Position_redundant</th>\n",
       "      <th>81 Window sequence</th>\n",
       "      <th>S or T</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>Q9UBP0</td>\n",
       "      <td>92</td>\n",
       "      <td>YFSYPLFVGFALLRLVAFHLGLLFVWLCQRFSRALMAAKRSSGAAP...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090565</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.265912</td>\n",
       "      <td>0.221255</td>\n",
       "      <td>0.272241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189914</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>-0.074416</td>\n",
       "      <td>0.055804</td>\n",
       "      <td>-0.027644</td>\n",
       "      <td>-0.171877</td>\n",
       "      <td>0.242402</td>\n",
       "      <td>-0.013881</td>\n",
       "      <td>-0.022422</td>\n",
       "      <td>0.076613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>616</td>\n",
       "      <td>A1L4H1</td>\n",
       "      <td>616</td>\n",
       "      <td>ISDPFSWSWIPGLGRDRDAWLPGELATKPSASVTASVLEKTTTKAP...</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.399097</td>\n",
       "      <td>0.180620</td>\n",
       "      <td>-0.011469</td>\n",
       "      <td>0.183442</td>\n",
       "      <td>0.480593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057433</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>0.333472</td>\n",
       "      <td>-0.083524</td>\n",
       "      <td>0.054328</td>\n",
       "      <td>-0.456104</td>\n",
       "      <td>-0.221937</td>\n",
       "      <td>0.233516</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>-0.186329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>447</td>\n",
       "      <td>Q4KMG0</td>\n",
       "      <td>447</td>\n",
       "      <td>IITAPVSAKVADGDFVTLSCNASGLPVPVIRWYDSHGLITSHPSQV...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.224062</td>\n",
       "      <td>0.048616</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.029075</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013840</td>\n",
       "      <td>0.103943</td>\n",
       "      <td>0.244616</td>\n",
       "      <td>-0.073571</td>\n",
       "      <td>0.115669</td>\n",
       "      <td>0.126064</td>\n",
       "      <td>0.114876</td>\n",
       "      <td>-0.263312</td>\n",
       "      <td>0.215138</td>\n",
       "      <td>0.056720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234</td>\n",
       "      <td>P05121</td>\n",
       "      <td>234</td>\n",
       "      <td>FNGQWKTPFPDSSTHRRLFHKSDGSTVSVPMMAQTNKFNYTEFTTP...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.119445</td>\n",
       "      <td>0.121850</td>\n",
       "      <td>0.369377</td>\n",
       "      <td>0.369897</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115018</td>\n",
       "      <td>-0.155688</td>\n",
       "      <td>0.051944</td>\n",
       "      <td>0.058649</td>\n",
       "      <td>0.276462</td>\n",
       "      <td>-0.187079</td>\n",
       "      <td>-0.021619</td>\n",
       "      <td>0.214745</td>\n",
       "      <td>-0.043378</td>\n",
       "      <td>-0.120113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>Q07960</td>\n",
       "      <td>130</td>\n",
       "      <td>ACRMPPSHQLDHSKLLGYLKHTLDQYVESDYTLLYLHHGLTSDNKP...</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.029804</td>\n",
       "      <td>-0.012195</td>\n",
       "      <td>0.098315</td>\n",
       "      <td>-0.187430</td>\n",
       "      <td>0.174959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062594</td>\n",
       "      <td>0.243982</td>\n",
       "      <td>-0.192061</td>\n",
       "      <td>0.074429</td>\n",
       "      <td>0.060254</td>\n",
       "      <td>-0.303652</td>\n",
       "      <td>0.218557</td>\n",
       "      <td>-0.179121</td>\n",
       "      <td>-0.222627</td>\n",
       "      <td>-0.108745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114139</th>\n",
       "      <td>954</td>\n",
       "      <td>Q8TEM1</td>\n",
       "      <td>954</td>\n",
       "      <td>LRIREGSGYFFLNTSTADVVKVAYQEARGVAMVHPLLPGSSTIMIH...</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.373295</td>\n",
       "      <td>0.188129</td>\n",
       "      <td>-0.315040</td>\n",
       "      <td>-0.151053</td>\n",
       "      <td>-0.373331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>0.277940</td>\n",
       "      <td>-0.331345</td>\n",
       "      <td>-0.313045</td>\n",
       "      <td>0.199881</td>\n",
       "      <td>0.142525</td>\n",
       "      <td>0.323091</td>\n",
       "      <td>0.520124</td>\n",
       "      <td>-0.196773</td>\n",
       "      <td>0.170163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114140</th>\n",
       "      <td>45</td>\n",
       "      <td>P12109</td>\n",
       "      <td>45</td>\n",
       "      <td>RALLPLLLQACWTAAQDEPETPRAVAFQDCPVDLFFVLDTSESVAL...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.274927</td>\n",
       "      <td>-0.068854</td>\n",
       "      <td>0.322756</td>\n",
       "      <td>0.071589</td>\n",
       "      <td>-0.260691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.350866</td>\n",
       "      <td>-0.266978</td>\n",
       "      <td>-0.229258</td>\n",
       "      <td>-0.064659</td>\n",
       "      <td>-0.413040</td>\n",
       "      <td>-0.067693</td>\n",
       "      <td>0.026467</td>\n",
       "      <td>0.048538</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114141</th>\n",
       "      <td>721</td>\n",
       "      <td>Q685J3</td>\n",
       "      <td>721</td>\n",
       "      <td>STYTEGSTPLTSMPVNTTLVASSEASTLSTTPVDTSTPVTTSTEAS...</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.079311</td>\n",
       "      <td>0.448357</td>\n",
       "      <td>0.206784</td>\n",
       "      <td>0.247052</td>\n",
       "      <td>0.289342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146359</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>0.229490</td>\n",
       "      <td>-0.253621</td>\n",
       "      <td>0.407581</td>\n",
       "      <td>-0.132414</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>-0.176205</td>\n",
       "      <td>0.063649</td>\n",
       "      <td>0.103462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114142</th>\n",
       "      <td>11864</td>\n",
       "      <td>Q8WXI7</td>\n",
       "      <td>11864</td>\n",
       "      <td>TLLVTGTSRVDLSPTASPGVSAKTAPLSTHPGTETSTMIPTSTLSL...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.083592</td>\n",
       "      <td>-0.027967</td>\n",
       "      <td>0.200502</td>\n",
       "      <td>0.299424</td>\n",
       "      <td>0.469784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243491</td>\n",
       "      <td>0.622060</td>\n",
       "      <td>0.429536</td>\n",
       "      <td>-0.161372</td>\n",
       "      <td>0.220750</td>\n",
       "      <td>-0.265639</td>\n",
       "      <td>-0.212433</td>\n",
       "      <td>-0.285282</td>\n",
       "      <td>0.537182</td>\n",
       "      <td>0.153588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114143</th>\n",
       "      <td>257</td>\n",
       "      <td>Q9NU53</td>\n",
       "      <td>257</td>\n",
       "      <td>DEDVLPGKLPETPLRAEPPSSYKVMCQWMEKFRKDLCRFWSNVFPV...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.197303</td>\n",
       "      <td>-0.140666</td>\n",
       "      <td>0.085049</td>\n",
       "      <td>0.336462</td>\n",
       "      <td>-0.165136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088210</td>\n",
       "      <td>0.059169</td>\n",
       "      <td>0.021975</td>\n",
       "      <td>-0.043889</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>-0.093678</td>\n",
       "      <td>0.302321</td>\n",
       "      <td>0.136606</td>\n",
       "      <td>-0.157297</td>\n",
       "      <td>-0.006274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114144 rows × 1029 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Position     PID  Position_redundant  \\\n",
       "0             92  Q9UBP0                  92   \n",
       "1            616  A1L4H1                 616   \n",
       "2            447  Q4KMG0                 447   \n",
       "3            234  P05121                 234   \n",
       "4            130  Q07960                 130   \n",
       "...          ...     ...                 ...   \n",
       "114139       954  Q8TEM1                 954   \n",
       "114140        45  P12109                  45   \n",
       "114141       721  Q685J3                 721   \n",
       "114142     11864  Q8WXI7               11864   \n",
       "114143       257  Q9NU53                 257   \n",
       "\n",
       "                                       81 Window sequence S or T         1  \\\n",
       "0       YFSYPLFVGFALLRLVAFHLGLLFVWLCQRFSRALMAAKRSSGAAP...      S  0.090565   \n",
       "1       ISDPFSWSWIPGLGRDRDAWLPGELATKPSASVTASVLEKTTTKAP...      T -0.399097   \n",
       "2       IITAPVSAKVADGDFVTLSCNASGLPVPVIRWYDSHGLITSHPSQV...      S  0.224062   \n",
       "3       FNGQWKTPFPDSSTHRRLFHKSDGSTVSVPMMAQTNKFNYTEFTTP...      T  0.119445   \n",
       "4       ACRMPPSHQLDHSKLLGYLKHTLDQYVESDYTLLYLHHGLTSDNKP...      T -0.029804   \n",
       "...                                                   ...    ...       ...   \n",
       "114139  LRIREGSGYFFLNTSTADVVKVAYQEARGVAMVHPLLPGSSTIMIH...      S -0.373295   \n",
       "114140  RALLPLLLQACWTAAQDEPETPRAVAFQDCPVDLFFVLDTSESVAL...      S  0.274927   \n",
       "114141  STYTEGSTPLTSMPVNTTLVASSEASTLSTTPVDTSTPVTTSTEAS...      T -0.079311   \n",
       "114142  TLLVTGTSRVDLSPTASPGVSAKTAPLSTHPGTETSTMIPTSTLSL...      T  0.083592   \n",
       "114143  DEDVLPGKLPETPLRAEPPSSYKVMCQWMEKFRKDLCRFWSNVFPV...      S  0.197303   \n",
       "\n",
       "               2         3         4         5  ...      1015      1016  \\\n",
       "0       0.003615  0.265912  0.221255  0.272241  ...  0.189914  0.008025   \n",
       "1       0.180620 -0.011469  0.183442  0.480593  ... -0.057433 -0.002528   \n",
       "2       0.048616  0.022553 -0.029075  0.030947  ...  0.013840  0.103943   \n",
       "3       0.121850  0.369377  0.369897  0.012500  ... -0.115018 -0.155688   \n",
       "4      -0.012195  0.098315 -0.187430  0.174959  ... -0.062594  0.243982   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "114139  0.188129 -0.315040 -0.151053 -0.373331  ...  0.046646  0.277940   \n",
       "114140 -0.068854  0.322756  0.071589 -0.260691  ...  0.002543  0.350866   \n",
       "114141  0.448357  0.206784  0.247052  0.289342  ...  0.146359  0.042260   \n",
       "114142 -0.027967  0.200502  0.299424  0.469784  ... -0.243491  0.622060   \n",
       "114143 -0.140666  0.085049  0.336462 -0.165136  ... -0.088210  0.059169   \n",
       "\n",
       "            1017      1018      1019      1020      1021      1022      1023  \\\n",
       "0      -0.074416  0.055804 -0.027644 -0.171877  0.242402 -0.013881 -0.022422   \n",
       "1       0.333472 -0.083524  0.054328 -0.456104 -0.221937  0.233516  0.099400   \n",
       "2       0.244616 -0.073571  0.115669  0.126064  0.114876 -0.263312  0.215138   \n",
       "3       0.051944  0.058649  0.276462 -0.187079 -0.021619  0.214745 -0.043378   \n",
       "4      -0.192061  0.074429  0.060254 -0.303652  0.218557 -0.179121 -0.222627   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "114139 -0.331345 -0.313045  0.199881  0.142525  0.323091  0.520124 -0.196773   \n",
       "114140 -0.266978 -0.229258 -0.064659 -0.413040 -0.067693  0.026467  0.048538   \n",
       "114141  0.229490 -0.253621  0.407581 -0.132414 -0.034729 -0.176205  0.063649   \n",
       "114142  0.429536 -0.161372  0.220750 -0.265639 -0.212433 -0.285282  0.537182   \n",
       "114143  0.021975 -0.043889  0.000672 -0.093678  0.302321  0.136606 -0.157297   \n",
       "\n",
       "            1024  \n",
       "0       0.076613  \n",
       "1      -0.186329  \n",
       "2       0.056720  \n",
       "3      -0.120113  \n",
       "4      -0.108745  \n",
       "...          ...  \n",
       "114139  0.170163  \n",
       "114140  0.494500  \n",
       "114141  0.103462  \n",
       "114142  0.153588  \n",
       "114143 -0.006274  \n",
       "\n",
       "[114144 rows x 1029 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d137bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>PID</th>\n",
       "      <th>Position_redundant</th>\n",
       "      <th>81 Window sequence</th>\n",
       "      <th>S or T</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4060</td>\n",
       "      <td>Q8WXI7</td>\n",
       "      <td>4060</td>\n",
       "      <td>VSPYMDTSSTTQTSIISSPGSTAITKGPRTEITSSKRISSSFLAQS...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>-0.118263</td>\n",
       "      <td>0.177780</td>\n",
       "      <td>0.246446</td>\n",
       "      <td>-0.082226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150221</td>\n",
       "      <td>0.583051</td>\n",
       "      <td>-0.020838</td>\n",
       "      <td>-0.194725</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>-0.038010</td>\n",
       "      <td>0.268013</td>\n",
       "      <td>-0.160448</td>\n",
       "      <td>0.313330</td>\n",
       "      <td>0.031695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2733</td>\n",
       "      <td>Q02817</td>\n",
       "      <td>2733</td>\n",
       "      <td>TGTQTPTSTPISTTTTVTPTATPTGTQTPTLTPITTTTTVTSTPTP...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.526688</td>\n",
       "      <td>0.228523</td>\n",
       "      <td>0.414098</td>\n",
       "      <td>0.151443</td>\n",
       "      <td>0.523440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490214</td>\n",
       "      <td>0.017934</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>-0.074319</td>\n",
       "      <td>0.311159</td>\n",
       "      <td>-0.362215</td>\n",
       "      <td>-0.271415</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.106888</td>\n",
       "      <td>0.164128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>P15514</td>\n",
       "      <td>52</td>\n",
       "      <td>VLSLLILGSGHYAAGLDLNDTYSGKREPFSGDHSADGFEVTSRSEM...</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.404962</td>\n",
       "      <td>0.329316</td>\n",
       "      <td>0.109950</td>\n",
       "      <td>-0.017265</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>-0.008204</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>0.071839</td>\n",
       "      <td>0.042044</td>\n",
       "      <td>-0.232947</td>\n",
       "      <td>-0.516805</td>\n",
       "      <td>-0.376891</td>\n",
       "      <td>0.200629</td>\n",
       "      <td>-0.278637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3737</td>\n",
       "      <td>Q02817</td>\n",
       "      <td>3737</td>\n",
       "      <td>TPTPTPTGTQTPTSTPITTTTTVTPTPTPTGTQTPTPTPISTTSTV...</td>\n",
       "      <td>S</td>\n",
       "      <td>0.115557</td>\n",
       "      <td>0.386895</td>\n",
       "      <td>0.535525</td>\n",
       "      <td>0.383236</td>\n",
       "      <td>-0.216051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198823</td>\n",
       "      <td>0.300182</td>\n",
       "      <td>0.126487</td>\n",
       "      <td>-0.285474</td>\n",
       "      <td>0.224651</td>\n",
       "      <td>-0.123275</td>\n",
       "      <td>0.157720</td>\n",
       "      <td>-0.332858</td>\n",
       "      <td>-0.044721</td>\n",
       "      <td>-0.287276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>O00461</td>\n",
       "      <td>270</td>\n",
       "      <td>QVAEYKQLKDTLNRIPSLRKPDPAEQQNVTQVAHSPQGYNTAREKP...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>-0.081142</td>\n",
       "      <td>0.038349</td>\n",
       "      <td>-0.002521</td>\n",
       "      <td>-0.134780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.072490</td>\n",
       "      <td>0.102203</td>\n",
       "      <td>0.094363</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>-0.011620</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.071256</td>\n",
       "      <td>-0.066625</td>\n",
       "      <td>0.169510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>292</td>\n",
       "      <td>Q14242</td>\n",
       "      <td>292</td>\n",
       "      <td>QTTPLAAMEALSTEPSATEALSMEPTTKRGLFIPFSVSSVTHKGIP...</td>\n",
       "      <td>T</td>\n",
       "      <td>-0.143447</td>\n",
       "      <td>0.091008</td>\n",
       "      <td>0.246226</td>\n",
       "      <td>-0.029177</td>\n",
       "      <td>0.333892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279839</td>\n",
       "      <td>-0.087998</td>\n",
       "      <td>0.394984</td>\n",
       "      <td>0.096512</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.536879</td>\n",
       "      <td>-0.548743</td>\n",
       "      <td>-0.279505</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>-0.044220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>451</td>\n",
       "      <td>P15941</td>\n",
       "      <td>451</td>\n",
       "      <td>TAPPAHGVTSAPDTRPAPGSTAPPAHGVTSAPDTRPAPGSTAPPAH...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>0.219554</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>0.233328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083353</td>\n",
       "      <td>0.139198</td>\n",
       "      <td>-0.114918</td>\n",
       "      <td>-0.232843</td>\n",
       "      <td>0.488561</td>\n",
       "      <td>-0.332134</td>\n",
       "      <td>-0.445608</td>\n",
       "      <td>0.203515</td>\n",
       "      <td>0.145742</td>\n",
       "      <td>0.257425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4882</th>\n",
       "      <td>2294</td>\n",
       "      <td>Q02817</td>\n",
       "      <td>2294</td>\n",
       "      <td>TPTGTQTPTSTPISTTTTVTPTPTPTGTQTPTPTPISTTTTVTPTP...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.584030</td>\n",
       "      <td>0.198676</td>\n",
       "      <td>0.033160</td>\n",
       "      <td>0.123643</td>\n",
       "      <td>0.531183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478282</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.143494</td>\n",
       "      <td>-0.107475</td>\n",
       "      <td>0.515316</td>\n",
       "      <td>-0.232197</td>\n",
       "      <td>-0.323919</td>\n",
       "      <td>-0.139580</td>\n",
       "      <td>0.341235</td>\n",
       "      <td>0.464046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>530</td>\n",
       "      <td>P02768</td>\n",
       "      <td>530</td>\n",
       "      <td>KTPVSDRVTKCCTESLVNRRPCFSALEVDETYVPKEFNAETFTFHA...</td>\n",
       "      <td>T</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>0.125332</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.058702</td>\n",
       "      <td>-0.168642</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178552</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>-0.073922</td>\n",
       "      <td>0.169234</td>\n",
       "      <td>0.156390</td>\n",
       "      <td>-0.276549</td>\n",
       "      <td>-0.240751</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>-0.074785</td>\n",
       "      <td>-0.145535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>127</td>\n",
       "      <td>Q12965</td>\n",
       "      <td>127</td>\n",
       "      <td>HIYALADNMYRNMIIDRENQCVIISGESGAGKTVAAKYIMSYISRV...</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.030142</td>\n",
       "      <td>0.295818</td>\n",
       "      <td>-0.186532</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>-0.141690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071001</td>\n",
       "      <td>-0.028582</td>\n",
       "      <td>-0.231510</td>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>-0.203327</td>\n",
       "      <td>0.248928</td>\n",
       "      <td>-0.081589</td>\n",
       "      <td>-0.336975</td>\n",
       "      <td>-0.252609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4885 rows × 1029 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Position     PID  Position_redundant  \\\n",
       "0         4060  Q8WXI7                4060   \n",
       "1         2733  Q02817                2733   \n",
       "2           52  P15514                  52   \n",
       "3         3737  Q02817                3737   \n",
       "4          270  O00461                 270   \n",
       "...        ...     ...                 ...   \n",
       "4880       292  Q14242                 292   \n",
       "4881       451  P15941                 451   \n",
       "4882      2294  Q02817                2294   \n",
       "4883       530  P02768                 530   \n",
       "4884       127  Q12965                 127   \n",
       "\n",
       "                                     81 Window sequence S or T         1  \\\n",
       "0     VSPYMDTSSTTQTSIISSPGSTAITKGPRTEITSSKRISSSFLAQS...      S  0.022986   \n",
       "1     TGTQTPTSTPISTTTTVTPTATPTGTQTPTLTPITTTTTVTSTPTP...      T  0.526688   \n",
       "2     VLSLLILGSGHYAAGLDLNDTYSGKREPFSGDHSADGFEVTSRSEM...      T -0.404962   \n",
       "3     TPTPTPTGTQTPTSTPITTTTTVTPTPTPTGTQTPTPTPISTTSTV...      S  0.115557   \n",
       "4     QVAEYKQLKDTLNRIPSLRKPDPAEQQNVTQVAHSPQGYNTAREKP...      T  0.103378   \n",
       "...                                                 ...    ...       ...   \n",
       "4880  QTTPLAAMEALSTEPSATEALSMEPTTKRGLFIPFSVSSVTHKGIP...      T -0.143447   \n",
       "4881  TAPPAHGVTSAPDTRPAPGSTAPPAHGVTSAPDTRPAPGSTAPPAH...      T  0.333800   \n",
       "4882  TPTGTQTPTSTPISTTTTVTPTPTPTGTQTPTPTPISTTTTVTPTP...      T  0.584030   \n",
       "4883  KTPVSDRVTKCCTESLVNRRPCFSALEVDETYVPKEFNAETFTFHA...      T  0.047491   \n",
       "4884  HIYALADNMYRNMIIDRENQCVIISGESGAGKTVAAKYIMSYISRV...      S -0.030142   \n",
       "\n",
       "             2         3         4         5  ...      1015      1016  \\\n",
       "0    -0.118263  0.177780  0.246446 -0.082226  ... -0.150221  0.583051   \n",
       "1     0.228523  0.414098  0.151443  0.523440  ...  0.490214  0.017934   \n",
       "2     0.329316  0.109950 -0.017265  0.073663  ... -0.018510 -0.008204   \n",
       "3     0.386895  0.535525  0.383236 -0.216051  ...  0.198823  0.300182   \n",
       "4    -0.081142  0.038349 -0.002521 -0.134780  ...  0.003350  0.072490   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "4880  0.091008  0.246226 -0.029177  0.333892  ... -0.279839 -0.087998   \n",
       "4881  0.024173  0.219554  0.103433  0.233328  ... -0.083353  0.139198   \n",
       "4882  0.198676  0.033160  0.123643  0.531183  ...  0.478282  0.151042   \n",
       "4883  0.125332  0.046872  0.058702 -0.168642  ... -0.178552  0.040738   \n",
       "4884  0.295818 -0.186532  0.057203 -0.141690  ... -0.071001 -0.028582   \n",
       "\n",
       "          1017      1018      1019      1020      1021      1022      1023  \\\n",
       "0    -0.020838 -0.194725  0.039820 -0.038010  0.268013 -0.160448  0.313330   \n",
       "1     0.195478 -0.074319  0.311159 -0.362215 -0.271415  0.040819  0.106888   \n",
       "2     0.039515  0.071839  0.042044 -0.232947 -0.516805 -0.376891  0.200629   \n",
       "3     0.126487 -0.285474  0.224651 -0.123275  0.157720 -0.332858 -0.044721   \n",
       "4     0.102203  0.094363  0.133203 -0.011620 -0.009290  0.071256 -0.066625   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4880  0.394984  0.096512 -0.036989 -0.536879 -0.548743 -0.279505  0.005524   \n",
       "4881 -0.114918 -0.232843  0.488561 -0.332134 -0.445608  0.203515  0.145742   \n",
       "4882  0.143494 -0.107475  0.515316 -0.232197 -0.323919 -0.139580  0.341235   \n",
       "4883 -0.073922  0.169234  0.156390 -0.276549 -0.240751  0.020197 -0.074785   \n",
       "4884 -0.231510  0.036151  0.009065 -0.203327  0.248928 -0.081589 -0.336975   \n",
       "\n",
       "          1024  \n",
       "0     0.031695  \n",
       "1     0.164128  \n",
       "2    -0.278637  \n",
       "3    -0.287276  \n",
       "4     0.169510  \n",
       "...        ...  \n",
       "4880 -0.044220  \n",
       "4881  0.257425  \n",
       "4882  0.464046  \n",
       "4883 -0.145535  \n",
       "4884 -0.252609  \n",
       "\n",
       "[4885 rows x 1029 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50e7c26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119029, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66557ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119029,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331e2e2",
   "metadata": {},
   "source": [
    "# Do 10 fold Cross Validation for 2 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbe013aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = random.sample(range(1, 1000000), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e836de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  284827\n",
      "(119029, 1024)\n",
      "(119029,)\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5289\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6672 - accuracy: 0.5648\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6491 - accuracy: 0.6016\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6309 - accuracy: 0.6368\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6183 - accuracy: 0.6593\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6075 - accuracy: 0.6775\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5982 - accuracy: 0.6901\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5890 - accuracy: 0.7050\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5794 - accuracy: 0.7136\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5687 - accuracy: 0.7224\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.7273\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5579 - accuracy: 0.7302\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5505 - accuracy: 0.7366\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5449 - accuracy: 0.7392\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5389 - accuracy: 0.7455\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5316 - accuracy: 0.7511\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.5269 - accuracy: 0.7584\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5205 - accuracy: 0.7578\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5157 - accuracy: 0.7629\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5126 - accuracy: 0.7682\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 1s 23ms/step - loss: 0.5082 - accuracy: 0.7712\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5078 - accuracy: 0.7687\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4975 - accuracy: 0.7748\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.4935 - accuracy: 0.7791\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.4919 - accuracy: 0.7805\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4839 - accuracy: 0.7847\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4819 - accuracy: 0.7847\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4820 - accuracy: 0.7866\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4783 - accuracy: 0.7915\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4699 - accuracy: 0.7930\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4780 - accuracy: 0.7868\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4730 - accuracy: 0.7910\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.7936\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4571 - accuracy: 0.8045\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4597 - accuracy: 0.7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6510756164403059\n",
      "Confusion Matrix : \n",
      " [[418  71]\n",
      " [100 388]]\n",
      "Accuracy on test set:    0.8249744114636642\n",
      "Sensitivity:    0.7950819672131147 \t Specificity:    0.8548057259713702\n",
      "Area Under Curve:    0.8249438465922424\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.5545\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.6573 - accuracy: 0.5889\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6407 - accuracy: 0.6196\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6219 - accuracy: 0.6527\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6094 - accuracy: 0.6669\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6000 - accuracy: 0.6791\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5886 - accuracy: 0.6975\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5858 - accuracy: 0.7021\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5735 - accuracy: 0.7142\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5708 - accuracy: 0.7200\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5646 - accuracy: 0.7241\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5576 - accuracy: 0.7344\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5478 - accuracy: 0.7404\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5437 - accuracy: 0.7426\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5410 - accuracy: 0.7446\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5356 - accuracy: 0.7472\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.5278 - accuracy: 0.7534\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5239 - accuracy: 0.7553\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5204 - accuracy: 0.7621\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5138 - accuracy: 0.7687\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5108 - accuracy: 0.7682\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5087 - accuracy: 0.7686\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5021 - accuracy: 0.7745\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4939 - accuracy: 0.7768\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4964 - accuracy: 0.7785\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4936 - accuracy: 0.7782\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4943 - accuracy: 0.7818\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.4839 - accuracy: 0.7839\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4850 - accuracy: 0.7846\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4802 - accuracy: 0.7845\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4743 - accuracy: 0.7892\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4692 - accuracy: 0.7922\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4734 - accuracy: 0.7871\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4684 - accuracy: 0.7948\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4637 - accuracy: 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6271562161119743\n",
      "Confusion Matrix : \n",
      " [[416  73]\n",
      " [110 378]]\n",
      "Accuracy on test set:    0.812691914022518\n",
      "Sensitivity:    0.7745901639344263 \t Specificity:    0.8507157464212679\n",
      "Area Under Curve:    0.8126529551778471\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.7165 - accuracy: 0.5219\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6718 - accuracy: 0.5761\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6472 - accuracy: 0.6199\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6334 - accuracy: 0.6368\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6188 - accuracy: 0.6613\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6054 - accuracy: 0.6767\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5950 - accuracy: 0.6944\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5870 - accuracy: 0.6951\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5797 - accuracy: 0.7048\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5683 - accuracy: 0.7169\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5695 - accuracy: 0.7150\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5538 - accuracy: 0.7276\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5547 - accuracy: 0.7305\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5440 - accuracy: 0.7398\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5383 - accuracy: 0.7480\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5385 - accuracy: 0.7471\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5282 - accuracy: 0.7557\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5227 - accuracy: 0.7592\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5224 - accuracy: 0.7621\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5196 - accuracy: 0.7615\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5103 - accuracy: 0.7663\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.5113 - accuracy: 0.7653\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.7755\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.5046 - accuracy: 0.7749\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5000 - accuracy: 0.7715\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.4912 - accuracy: 0.7772\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4934 - accuracy: 0.7765\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4932 - accuracy: 0.7785\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4869 - accuracy: 0.7815\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4802 - accuracy: 0.7841\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4812 - accuracy: 0.7876\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4730 - accuracy: 0.7964\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 0.4727 - accuracy: 0.7914\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.7873\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4640 - accuracy: 0.7965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.5982323628242714\n",
      "Confusion Matrix : \n",
      " [[408  81]\n",
      " [116 372]]\n",
      "Accuracy on test set:    0.7983623336745138\n",
      "Sensitivity:    0.7622950819672131 \t Specificity:    0.8343558282208589\n",
      "Area Under Curve:    0.7983254550940361\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.7203 - accuracy: 0.5104\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.6890 - accuracy: 0.5459\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.5970\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6467 - accuracy: 0.6296\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.6277 - accuracy: 0.6639\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6204 - accuracy: 0.6711\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6073 - accuracy: 0.6865\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5947 - accuracy: 0.7033\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5871 - accuracy: 0.7076\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5782 - accuracy: 0.7134\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5691 - accuracy: 0.7200\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5659 - accuracy: 0.7256\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.7333\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5518 - accuracy: 0.7354\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5468 - accuracy: 0.7382\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5418 - accuracy: 0.7448\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5343 - accuracy: 0.7505\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5315 - accuracy: 0.7534\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5237 - accuracy: 0.7607\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5193 - accuracy: 0.7599\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5182 - accuracy: 0.7679\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5122 - accuracy: 0.7685\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7748\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5068 - accuracy: 0.7730\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5002 - accuracy: 0.7748\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4963 - accuracy: 0.7799\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.4895 - accuracy: 0.7831\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4897 - accuracy: 0.7854\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4831 - accuracy: 0.7896\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4842 - accuracy: 0.7886\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.4790 - accuracy: 0.7884\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4804 - accuracy: 0.7862\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4728 - accuracy: 0.7988\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4688 - accuracy: 0.7950\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4696 - accuracy: 0.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.5824106751348839\n",
      "Confusion Matrix : \n",
      " [[389 100]\n",
      " [104 384]]\n",
      "Accuracy on test set:    0.7911975435005117\n",
      "Sensitivity:    0.7868852459016393 \t Specificity:    0.7955010224948875\n",
      "Area Under Curve:    0.7911931341982634\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6999 - accuracy: 0.5346\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6733 - accuracy: 0.5742\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6573 - accuracy: 0.6009\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6384 - accuracy: 0.6351\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6266 - accuracy: 0.6482\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.6140 - accuracy: 0.6758\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 1s 14ms/step - loss: 0.6042 - accuracy: 0.6809\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5962 - accuracy: 0.6909\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5876 - accuracy: 0.6999\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5803 - accuracy: 0.7094\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.5741 - accuracy: 0.7172\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5685 - accuracy: 0.7203\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5616 - accuracy: 0.7216\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5556 - accuracy: 0.7298\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5464 - accuracy: 0.7325\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5422 - accuracy: 0.7446\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.5346 - accuracy: 0.7511\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.5337 - accuracy: 0.7517\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5324 - accuracy: 0.7496\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.5271 - accuracy: 0.7555\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.5188 - accuracy: 0.7628\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5150 - accuracy: 0.7678\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5092 - accuracy: 0.7675\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5001 - accuracy: 0.7718\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.7785\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4970 - accuracy: 0.7789\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.7789\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4861 - accuracy: 0.7813\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4864 - accuracy: 0.7785\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4803 - accuracy: 0.7848\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4778 - accuracy: 0.7874\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4791 - accuracy: 0.7872\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4711 - accuracy: 0.7901\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4671 - accuracy: 0.7923\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6238436792535661\n",
      "Confusion Matrix : \n",
      " [[407  82]\n",
      " [102 386]]\n",
      "Accuracy on test set:    0.8116683725690891\n",
      "Sensitivity:    0.7909836065573771 \t Specificity:    0.8323108384458078\n",
      "Area Under Curve:    0.8116472225015924\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.6915 - accuracy: 0.5302\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6646 - accuracy: 0.5817\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6494 - accuracy: 0.6164\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6345 - accuracy: 0.6422\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6198 - accuracy: 0.6613\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.6097 - accuracy: 0.6751\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6001 - accuracy: 0.6943\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5922 - accuracy: 0.7018\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5843 - accuracy: 0.7067\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5739 - accuracy: 0.7194\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5693 - accuracy: 0.7250\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5631 - accuracy: 0.7277\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5590 - accuracy: 0.7257\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5506 - accuracy: 0.7396\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5430 - accuracy: 0.7466\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.5359 - accuracy: 0.7522\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5340 - accuracy: 0.7460\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5246 - accuracy: 0.7574\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5194 - accuracy: 0.7628\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5173 - accuracy: 0.7642\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5088 - accuracy: 0.7664\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5089 - accuracy: 0.7727\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5038 - accuracy: 0.7746\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4990 - accuracy: 0.7754\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4937 - accuracy: 0.7787\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4939 - accuracy: 0.7771\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4861 - accuracy: 0.7853\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4834 - accuracy: 0.7844\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.7862\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4776 - accuracy: 0.7884\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4739 - accuracy: 0.7907\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4715 - accuracy: 0.7902\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4695 - accuracy: 0.7963\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4648 - accuracy: 0.7985\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.4602 - accuracy: 0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6236835833301366\n",
      "Confusion Matrix : \n",
      " [[404  84]\n",
      " [100 389]]\n",
      "Accuracy on test set:    0.8116683725690891\n",
      "Sensitivity:    0.7955010224948875 \t Specificity:    0.8278688524590164\n",
      "Area Under Curve:    0.8116849374769519\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6968 - accuracy: 0.5113\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6748 - accuracy: 0.5441\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6577 - accuracy: 0.5641\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6459 - accuracy: 0.5858\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.6318 - accuracy: 0.6247\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6219 - accuracy: 0.6464\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6130 - accuracy: 0.6679\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.6085 - accuracy: 0.6811\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5967 - accuracy: 0.6924\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.7045\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5818 - accuracy: 0.7100\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.7230\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.7261\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5635 - accuracy: 0.7362\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5558 - accuracy: 0.7404\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5515 - accuracy: 0.7442\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5441 - accuracy: 0.7501\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5375 - accuracy: 0.7528\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5333 - accuracy: 0.7597\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.5275 - accuracy: 0.7597\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5209 - accuracy: 0.7665\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 0.7738\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5128 - accuracy: 0.7686\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5049 - accuracy: 0.7711\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4996 - accuracy: 0.7804\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4943 - accuracy: 0.7785\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4925 - accuracy: 0.7847\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4844 - accuracy: 0.7856\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.7890\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4806 - accuracy: 0.7938\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4735 - accuracy: 0.7907\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4720 - accuracy: 0.7888\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7947\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.7964\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.7979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.5509745915779896\n",
      "Confusion Matrix : \n",
      " [[411  77]\n",
      " [145 344]]\n",
      "Accuracy on test set:    0.7727737973387923\n",
      "Sensitivity:    0.7034764826175869 \t Specificity:    0.8422131147540983\n",
      "Area Under Curve:    0.7728447986858427\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5271\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6734 - accuracy: 0.5758\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6551 - accuracy: 0.6195\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.6409 - accuracy: 0.6447\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.6269 - accuracy: 0.6686\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.6157 - accuracy: 0.6780\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.6073 - accuracy: 0.6899\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5944 - accuracy: 0.7064\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5874 - accuracy: 0.7074\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5771 - accuracy: 0.7201\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5709 - accuracy: 0.7292\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5645 - accuracy: 0.7290\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5555 - accuracy: 0.7376\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5446 - accuracy: 0.7468\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5420 - accuracy: 0.7536\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5339 - accuracy: 0.7548\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5307 - accuracy: 0.7578\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5261 - accuracy: 0.7620\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5220 - accuracy: 0.7627\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5137 - accuracy: 0.7677\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5099 - accuracy: 0.7699\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5072 - accuracy: 0.7723\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5021 - accuracy: 0.7708\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4984 - accuracy: 0.7760\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4915 - accuracy: 0.7807\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4887 - accuracy: 0.7832\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 0.7866\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4830 - accuracy: 0.7872\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4786 - accuracy: 0.7886\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4760 - accuracy: 0.7904\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4774 - accuracy: 0.7918\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4684 - accuracy: 0.7967\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.4636 - accuracy: 0.7975\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4638 - accuracy: 0.8000\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4623 - accuracy: 0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6011143974984375\n",
      "Confusion Matrix : \n",
      " [[398  90]\n",
      " [105 384]]\n",
      "Accuracy on test set:    0.8004094165813715\n",
      "Sensitivity:    0.7852760736196319 \t Specificity:    0.8155737704918032\n",
      "Area Under Curve:    0.8004249220557175\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.7061 - accuracy: 0.5136\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6833 - accuracy: 0.5607\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6603 - accuracy: 0.6081\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.6298\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.6304 - accuracy: 0.6499\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6188 - accuracy: 0.6673\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6093 - accuracy: 0.6763\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5954 - accuracy: 0.6917\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5882 - accuracy: 0.7039\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5789 - accuracy: 0.7105\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5727 - accuracy: 0.7180\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5711 - accuracy: 0.7127\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5577 - accuracy: 0.7367\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5537 - accuracy: 0.7313\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.5491 - accuracy: 0.7347\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5438 - accuracy: 0.7417\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5381 - accuracy: 0.7446\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.5357 - accuracy: 0.7501\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.5250 - accuracy: 0.7611\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5221 - accuracy: 0.7592\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5166 - accuracy: 0.7646\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5132 - accuracy: 0.7708\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.5095 - accuracy: 0.7672\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5065 - accuracy: 0.7715\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 0.7766\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4962 - accuracy: 0.7780\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4936 - accuracy: 0.7821\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4933 - accuracy: 0.7838\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4887 - accuracy: 0.7822\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4854 - accuracy: 0.7820\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4811 - accuracy: 0.7849\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4750 - accuracy: 0.7907\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4757 - accuracy: 0.7948\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4732 - accuracy: 0.7884\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4671 - accuracy: 0.7971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.5955432781312355\n",
      "Confusion Matrix : \n",
      " [[402  86]\n",
      " [112 377]]\n",
      "Accuracy on test set:    0.797338792221085\n",
      "Sensitivity:    0.7709611451942741 \t Specificity:    0.8237704918032787\n",
      "Area Under Curve:    0.7973658184987764\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6996 - accuracy: 0.5105\n",
      "Epoch 2/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6802 - accuracy: 0.5344\n",
      "Epoch 3/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6614 - accuracy: 0.5575\n",
      "Epoch 4/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.6435 - accuracy: 0.5821\n",
      "Epoch 5/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6349 - accuracy: 0.6079\n",
      "Epoch 6/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6232 - accuracy: 0.6369\n",
      "Epoch 7/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.6120 - accuracy: 0.6603\n",
      "Epoch 8/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6053 - accuracy: 0.6761\n",
      "Epoch 9/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5971 - accuracy: 0.6863\n",
      "Epoch 10/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5876 - accuracy: 0.6990\n",
      "Epoch 11/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5791 - accuracy: 0.7130\n",
      "Epoch 12/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5725 - accuracy: 0.7194\n",
      "Epoch 13/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5640 - accuracy: 0.7191\n",
      "Epoch 14/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5581 - accuracy: 0.7326\n",
      "Epoch 15/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5510 - accuracy: 0.7344\n",
      "Epoch 16/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5477 - accuracy: 0.7385\n",
      "Epoch 17/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5398 - accuracy: 0.7480\n",
      "Epoch 18/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5324 - accuracy: 0.7547\n",
      "Epoch 19/35\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5333 - accuracy: 0.7542\n",
      "Epoch 20/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.5280 - accuracy: 0.7557\n",
      "Epoch 21/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5166 - accuracy: 0.7602\n",
      "Epoch 22/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5174 - accuracy: 0.7625\n",
      "Epoch 23/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5153 - accuracy: 0.7631\n",
      "Epoch 24/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.5135 - accuracy: 0.7678\n",
      "Epoch 25/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5059 - accuracy: 0.7690\n",
      "Epoch 26/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.7779\n",
      "Epoch 27/35\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4964 - accuracy: 0.7769\n",
      "Epoch 28/35\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4940 - accuracy: 0.7795\n",
      "Epoch 29/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4881 - accuracy: 0.7788\n",
      "Epoch 30/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4849 - accuracy: 0.7843\n",
      "Epoch 31/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4811 - accuracy: 0.7852\n",
      "Epoch 32/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4803 - accuracy: 0.7897\n",
      "Epoch 33/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4813 - accuracy: 0.7871\n",
      "Epoch 34/35\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.4783 - accuracy: 0.7888\n",
      "Epoch 35/35\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.4733 - accuracy: 0.7957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/software/SciPy-bundle/2019.10-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:338: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation :  0.6453241736438683\n",
      "Confusion Matrix : \n",
      " [[426  62]\n",
      " [113 376]]\n",
      "Accuracy on test set:    0.8208802456499488\n",
      "Sensitivity:    0.7689161554192229 \t Specificity:    0.8729508196721312\n",
      "Area Under Curve:    0.820933487545677\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 664,418\n",
      "Trainable params: 664,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Mean MCC = 0.6099 + 0.0288 and Mean AUC = 0.8042 + 0.0146\n",
      "Mean MCC:    0.6099358573946668\n",
      "Mean Sensitivity = 0.7734 + 0.0258 and Mean Specificity = 0.8350 + 0.0206 and Mean Accuracy = 0.8042 + 0.0146\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    seed = i\n",
    "    print(\"Seed : \",seed)\n",
    "    rus = RandomUnderSampler(random_state = seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full,y_train_full)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train,y_train)\n",
    "\n",
    "    print(X_train_full.shape)\n",
    "    print(y_train_full.shape)\n",
    "\n",
    "    MCC = []\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    cvscores, auc_scores, sn, sp, acc = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train, y_train):\n",
    "        xtrain, xval = X_train[train_index], X_train[test_index]\n",
    "        ytrain, yval = y_train[train_index], y_train[test_index]      \n",
    "\n",
    "        X_train_10= xtrain\n",
    "        Y_train_10=ytrain\n",
    "\n",
    "        Y_train_10 = tf.keras.utils.to_categorical(Y_train_10,2)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Input(shape=(1024,)))\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(32,activation='relu',name=\"Dense_1\"))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(2,activation='softmax',name=\"Dense_2\"))\n",
    "\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "        checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"ROC_ROC_Premise_Assumption.h5\", \n",
    "                                        monitor = 'val_accuracy',\n",
    "                                        verbose=0, \n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True)\n",
    "\n",
    "        reduce_lr_acc = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.001, patience=3, verbose=1, min_delta=1e-4, mode='max')\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=5,mode='max')\n",
    "\n",
    "        history = model.fit(X_train_10, Y_train_10,epochs=35,verbose=1,batch_size=256)\n",
    "\n",
    "        pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0,1)\n",
    "        plt.show()\n",
    "        plt.savefig('accuracy_loss@@@@@@@@@@@@@@@_curve.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "        Y_pred = model.predict(xval)\n",
    "        Y_pred = (Y_pred > 0.5)\n",
    "        y_pred = [np.argmax(y, axis=None, out=None) for y in Y_pred]\n",
    "        y_pred = np.array(y_pred)\n",
    "        print(\"Matthews Correlation : \",matthews_corrcoef(yval, y_pred))\n",
    "        print(\"Confusion Matrix : \\n\",confusion_matrix(yval, y_pred))\n",
    "        print(\"Accuracy on test set:   \",accuracy_score(yval, y_pred))\n",
    "\n",
    "        cm = confusion_matrix(yval, y_pred)\n",
    "\n",
    "        TP = cm[1][1]\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "\n",
    "        mcc = matthews_corrcoef(yval, y_pred)\n",
    "\n",
    "        acc.append(accuracy_score(yval, y_pred))\n",
    "\n",
    "        cvscores.append(mcc)\n",
    "\n",
    "        Sensitivity = TP/(TP+FN)\n",
    "\n",
    "        Specificity = TN/(TN+FP)\n",
    "\n",
    "        sn.append(Sensitivity)\n",
    "        sp.append(Specificity)\n",
    "\n",
    "        print(\"Sensitivity:   \",Sensitivity,\"\\t\",\"Specificity:   \",Specificity)\n",
    "\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(yval, y_pred)\n",
    "\n",
    "        roc_auc_test = auc(fpr,tpr)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Area Under Curve:   \",roc_auc_test)\n",
    "        auc_scores.append(roc_auc_test)   \n",
    "\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Mean MCC = %.4f + %.4f and Mean AUC = %.4f + %.4f\" % (np.mean(cvscores),np.std(cvscores),np.mean(auc_scores),np.std(auc_scores)))\n",
    "    MCC.append(np.mean(cvscores))\n",
    "\n",
    "    print(\"Mean MCC:   \",np.mean(MCC))\n",
    "    print(\"Mean Sensitivity = %.4f + %.4f and Mean Specificity = %.4f + %.4f and Mean Accuracy = %.4f + %.4f\" % (np.mean(sn),np.std(sn),np.mean(sp),np.std(sp),np.mean(acc),np.std(acc)))\n",
    "\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cca690",
   "metadata": {},
   "source": [
    "# Independent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "503cc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  882022\n",
      "Epoch 1/400\n",
      "35/35 [==============================] - 1s 24ms/step - loss: 0.5057 - accuracy: 0.7649 - val_loss: 0.4402 - val_accuracy: 0.7932\n",
      "Epoch 2/400\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.4067 - accuracy: 0.8310 - val_loss: 0.4038 - val_accuracy: 0.8025\n",
      "Epoch 3/400\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 0.3549 - accuracy: 0.8607 - val_loss: 0.3949 - val_accuracy: 0.8229\n",
      "Epoch 4/400\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.2980 - accuracy: 0.8826 - val_loss: 0.3884 - val_accuracy: 0.8403\n",
      "Epoch 5/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2555 - accuracy: 0.9041 - val_loss: 0.4303 - val_accuracy: 0.8291\n",
      "Epoch 6/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2139 - accuracy: 0.9185 - val_loss: 0.4488 - val_accuracy: 0.8332\n",
      "Epoch 7/400\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1801 - accuracy: 0.9326 - val_loss: 0.5432 - val_accuracy: 0.8301\n",
      "Epoch 8/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.1569 - accuracy: 0.9411 - val_loss: 0.5401 - val_accuracy: 0.8280\n",
      "Epoch 9/400\n",
      "33/35 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9502\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.1327 - accuracy: 0.9497 - val_loss: 0.6173 - val_accuracy: 0.8219\n",
      "Matthews Correlation :  0.5379341166207293\n",
      "Confusion Matrix : \n",
      " [[314  60]\n",
      " [115 259]]\n",
      "Accuracy on test set:    0.766042780748663\n",
      "Sensitivity:    0.6925133689839572 \t Specificity:    0.839572192513369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78       374\n",
      "           1       0.81      0.69      0.75       374\n",
      "\n",
      "    accuracy                           0.77       748\n",
      "   macro avg       0.77      0.77      0.76       748\n",
      "weighted avg       0.77      0.77      0.76       748\n",
      "\n",
      "Area Under Curve:    0.766042780748663\n",
      "Seed :  121291\n",
      "Epoch 1/400\n",
      "35/35 [==============================] - 2s 70ms/step - loss: 0.5205 - accuracy: 0.7546 - val_loss: 0.4266 - val_accuracy: 0.8025\n",
      "Epoch 2/400\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.4167 - accuracy: 0.8279 - val_loss: 0.4114 - val_accuracy: 0.8168\n",
      "Epoch 3/400\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.3550 - accuracy: 0.8569 - val_loss: 0.4089 - val_accuracy: 0.8199\n",
      "Epoch 4/400\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.3106 - accuracy: 0.8768 - val_loss: 0.3977 - val_accuracy: 0.8291\n",
      "Epoch 5/400\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.2675 - accuracy: 0.8970 - val_loss: 0.4480 - val_accuracy: 0.8209\n",
      "Epoch 6/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2140 - accuracy: 0.9199 - val_loss: 0.4863 - val_accuracy: 0.8260\n",
      "Epoch 7/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.1848 - accuracy: 0.9326 - val_loss: 0.5600 - val_accuracy: 0.8250\n",
      "Epoch 8/400\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.1584 - accuracy: 0.9413 - val_loss: 0.5797 - val_accuracy: 0.8188\n",
      "Epoch 9/400\n",
      "28/35 [=======================>......] - ETA: 0s - loss: 0.1313 - accuracy: 0.9489\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.1357 - accuracy: 0.9483 - val_loss: 0.6185 - val_accuracy: 0.8229\n",
      "Matthews Correlation :  0.5406930675425667\n",
      "Confusion Matrix : \n",
      " [[324  50]\n",
      " [126 248]]\n",
      "Accuracy on test set:    0.7647058823529411\n",
      "Sensitivity:    0.6631016042780749 \t Specificity:    0.8663101604278075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79       374\n",
      "           1       0.83      0.66      0.74       374\n",
      "\n",
      "    accuracy                           0.76       748\n",
      "   macro avg       0.78      0.76      0.76       748\n",
      "weighted avg       0.78      0.76      0.76       748\n",
      "\n",
      "Area Under Curve:    0.7647058823529412\n",
      "Seed :  757839\n",
      "Epoch 1/400\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.5185 - accuracy: 0.7554 - val_loss: 0.4598 - val_accuracy: 0.7871\n",
      "Epoch 2/400\n",
      "35/35 [==============================] - 0s 14ms/step - loss: 0.4066 - accuracy: 0.8327 - val_loss: 0.4348 - val_accuracy: 0.8045\n",
      "Epoch 3/400\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.3522 - accuracy: 0.8578 - val_loss: 0.4330 - val_accuracy: 0.8106\n",
      "Epoch 4/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.3119 - accuracy: 0.8738 - val_loss: 0.4413 - val_accuracy: 0.7994\n",
      "Epoch 5/400\n",
      "35/35 [==============================] - 0s 13ms/step - loss: 0.2627 - accuracy: 0.9020 - val_loss: 0.4825 - val_accuracy: 0.8178\n",
      "Epoch 6/400\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.2287 - accuracy: 0.9162 - val_loss: 0.4724 - val_accuracy: 0.8137\n",
      "Epoch 7/400\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.1921 - accuracy: 0.9285 - val_loss: 0.5678 - val_accuracy: 0.7973\n",
      "Epoch 8/400\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.1611 - accuracy: 0.9387 - val_loss: 0.6034 - val_accuracy: 0.7953\n",
      "Epoch 9/400\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.1367 - accuracy: 0.9480 - val_loss: 0.7030 - val_accuracy: 0.8025\n",
      "Epoch 10/400\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 0.1188 - accuracy: 0.9560 - val_loss: 0.7360 - val_accuracy: 0.8199\n",
      "Epoch 11/400\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.1081 - accuracy: 0.9595 - val_loss: 0.7864 - val_accuracy: 0.8055\n",
      "Epoch 12/400\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.1048 - accuracy: 0.9621 - val_loss: 0.8002 - val_accuracy: 0.8025\n",
      "Epoch 13/400\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.0930 - accuracy: 0.9660 - val_loss: 0.7944 - val_accuracy: 0.8086\n",
      "Epoch 14/400\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.0817 - accuracy: 0.9705 - val_loss: 0.8129 - val_accuracy: 0.8066\n",
      "Epoch 15/400\n",
      "30/35 [========================>.....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9698\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974512e-06.\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.0844 - accuracy: 0.9696 - val_loss: 0.8497 - val_accuracy: 0.8045\n",
      "Matthews Correlation :  0.579407146168862\n",
      "Confusion Matrix : \n",
      " [[310  64]\n",
      " [ 94 280]]\n",
      "Accuracy on test set:    0.7887700534759359\n",
      "Sensitivity:    0.7486631016042781 \t Specificity:    0.8288770053475936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80       374\n",
      "           1       0.81      0.75      0.78       374\n",
      "\n",
      "    accuracy                           0.79       748\n",
      "   macro avg       0.79      0.79      0.79       748\n",
      "weighted avg       0.79      0.79      0.79       748\n",
      "\n",
      "Area Under Curve:    0.7887700534759358\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/home/t326h379/OGP\")\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Input, Flatten, LSTM, Dropout, Bidirectional, LeakyReLU, Reshape, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# performance matrices\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import imblearn\n",
    "a = random.sample(range(1, 1000000), 10)\n",
    "\n",
    "# Training Dataset\n",
    "\n",
    "df_negative = pd.read_csv('Feature_Extraction_O_linked_Training_Negative_114307_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive = pd.read_csv('Feature_Extraction_O_linked_Training_Positive_4885_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive.columns = Header_name\n",
    "df_negative.columns = Header_name\n",
    "\n",
    "\n",
    "frames = [df_positive, df_negative]\n",
    "\n",
    "O_linked_training = pd.concat(frames,ignore_index = True)\n",
    "\n",
    "df_Train_array = O_linked_training.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Train_array = np.array(df_Train_array)\n",
    "\n",
    "X_train_full = df_Train_array\n",
    "\n",
    "y_train_full = np.array([1]*4885+[0]*114144)\n",
    "\n",
    "# Independent Test Dataset\n",
    "df_negative_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Negative_11466_Sites_less.txt',header=None)\n",
    "\n",
    "df_positive_test = pd.read_csv('Feature_Extraction_O_linked_Testing_Positive_375_Sites_less.txt',header=None)\n",
    "\n",
    "Header_name = [\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"]\n",
    "\n",
    "col_of_feature = [i for i in range(1,1025)]\n",
    "\n",
    "Header_name = Header_name + col_of_feature\n",
    "\n",
    "df_positive_test.columns = Header_name\n",
    "\n",
    "df_negative_test.columns = Header_name\n",
    "\n",
    "\n",
    "frames_test = [df_positive_test, df_negative_test]\n",
    "\n",
    "O_linked_testing = pd.concat(frames_test,ignore_index = True)\n",
    "\n",
    "df_Test_array = O_linked_testing.drop([\"Position\",\"PID\",\"Position_redundant\",\"81 Window sequence\",\"S or T\"],axis=1)\n",
    "df_Test_array = np.array(df_Test_array)\n",
    "\n",
    "X_test_full = df_Test_array\n",
    "\n",
    "y_test_full = np.array([1]*374+[0]*11466)\n",
    "\n",
    "# Training Starts From Here\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, auc\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Run the model for Three times and choose the best answer among them\n",
    "a = random.sample(range(1, 1000000), 3)\n",
    "\n",
    "for i in a:\n",
    "    seed = i\n",
    "    print(\"Seed : \", seed)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state = seed)\n",
    "    X_train, y_train = rus.fit_resample(X_train_full,y_train_full)\n",
    "\n",
    "    x_train, x_val, y_train_1, y_val = train_test_split(X_train, y_train,random_state =21, test_size=0.1)\n",
    "\n",
    "    y_train_1 = tf.keras.utils.to_categorical(y_train_1,2)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val,2)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(1024,)))\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(32,activation='relu',name=\"Dense_1\"))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(2,activation='softmax',name=\"Dense_2\"))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"ROC_ROC_Premise_Assumption.h5\", \n",
    "                                    monitor = 'val_accuracy',\n",
    "                                    verbose=0, \n",
    "                                    save_weights_only=False,\n",
    "                                    save_best_only=True)\n",
    "\n",
    "    reduce_lr_acc = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.001, patience=5, verbose=1, min_delta=1e-4, mode='max')\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=5,mode='max')\n",
    "\n",
    "    history = model.fit(x_train, y_train_1,epochs=400,verbose=1,batch_size=256,\n",
    "                            callbacks=[checkpointer,reduce_lr_acc, early_stopping],validation_data=(x_val, y_val))\n",
    "    \n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    rus = RandomUnderSampler(random_state = seed)\n",
    "    X_independent, y_independent = rus.fit_resample(X_test_full,y_test_full)\n",
    "    \n",
    "    Y_pred = model.predict(X_independent)\n",
    "    Y_pred = (Y_pred > 0.5)\n",
    "    y_pred = [np.argmax(y, axis=None, out=None) for y in Y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    confusion = confusion_matrix(y_independent,y_pred)\n",
    "\n",
    "    print(\"Matthews Correlation : \",matthews_corrcoef(y_independent, y_pred))\n",
    "    print(\"Confusion Matrix : \\n\",confusion_matrix(y_independent, y_pred))\n",
    "    print(\"Accuracy on test set:   \",accuracy_score(y_independent, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_independent, y_pred)\n",
    "\n",
    "    TP = cm[1][1]\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "\n",
    "    mcc = matthews_corrcoef(y_independent, y_pred)\n",
    "\n",
    "    Sensitivity = TP/(TP+FN)\n",
    "\n",
    "    Specificity = TN/(TN+FP)\n",
    "\n",
    "    print(\"Sensitivity:   \",Sensitivity,\"\\t\",\"Specificity:   \",Specificity)\n",
    "\n",
    "    print(classification_report(y_independent, y_pred))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_independent, y_pred)\n",
    "\n",
    "    roc_auc_test = auc(fpr,tpr)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Area Under Curve:   \",roc_auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a749d2",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4-TensorFlow-2.3.1 [jupyter_python]",
   "language": "python",
   "name": "sys_python_3.7.4-tensorflow-2.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
