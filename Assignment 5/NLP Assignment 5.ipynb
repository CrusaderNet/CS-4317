{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1). Explain the Backpropagation Algorithm in the provided image**\n",
    "\n",
    "We are going to use the computation of Gradient = local gradient * upstream gradient to update the weights. First we should compute the derivatives of each node's function, not including the + and * nodes. for the *-1 node, we have a function of a * x, and therefore a derivative of a. For the exp node, we have a function of exp, and therefore a derivative of exp. for the +1 node we have a function of c + x and therefore a derivative of 1. For the 1/x node, we have a function of 1/x and therefore a derivative of -1/x^2.\n",
    "\n",
    "What we will do next is apply the local gradient * upstream gradient function to do our backpropagation. All the way on the right at the output, we start with a 1.00. Next, we calculate the weight before the 1/x node. we do this as local gradient * upstream gradient, or (-1/(1.37)^2) * 1, which gives -0.53. The next weight we calculate will be the +1 node, and the calculation will be 1 * -0.53, which gives -0.53. The next node we calculate is the exp node, which is exp(-1) * -0.53 which gives -0.20. The next node is the *-1 node, calculated as -1 * -0.20 which gives 0.20. The next node splits into two nodes backwards, and it is a + node. + nodes are known as a gradient distributor, which means that the previous nodes new weights are the same as the weight after the + node. So, our w2 weight is now 0.2, and the other node which is another + node is 0.2. Since the next node is also a node distributor, it splits into two * nodes which both get the 0.2 weight from the previous node distributor and this node distributor. This leads us to two * nodes, which are gradient switchers. for our w0 node, we use the x0 node's current weight and multiply it by the upstream gradient, to get our new gradient for w0. This is -1.00 * 0.20, which is -0.20. We do the same for x0, which is 2.0 * 0.2 resulting in 0.4. This means that w0 gradient is -0.2, and x0 gradient is 0.4. We do the same for the other * node, which gives us w1 gradient as -0.4, and x1 gradient as -0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2). What are the purposes of a) max gate b) Multiplication gate c) adder gate d) What is a chain rule**\n",
    "\n",
    "- Max Gate: The max gate is a gradient router, which outputs only the larger of the values going in, and in backpropagation only the higher of the paths recieve the gradient while the others recieve 0.\n",
    "- Multiplication Gate: This is a gradient switcher, in forward propagation it multiplies the inputs, however in backpropagation it uses the other input * the upstream gradient to calculate a gradient.\n",
    "- Adder Gate: This gate adds the inputs together in forward propagation, and in backward propagation it assigns the upstream gradient to each backward gradient.\n",
    "- Chain Rule: The chain rule allows you to compute gradients of composite functions, as df/dx = df/dq * dq/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3). Please explain the backpropagation algorithm used to update the weights on the following computational graph.**\n",
    "\n",
    "For the backpropagation of this graph, we start with df/df which is equal to 1. Then we have a * gate, which uses the opposite inputs * df/df gradient to assign new weights. So therefore Z weight is 3 * 1 = 3, and the q weight is -4 * 1 = -4. Then, we have an add + gate, which is a gradient distributor. It assigns the gradients as equal to the q gradient, so both the X gradient and the Y gradient are = -4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4). What are supervised, unsupervised, and self-supervised algorithms? Please explain each with relevant examples.**\n",
    "- Supervised Algorithms: These algorithms and models learn from labeled data(each input provided in the training dataset come with a known output or label), in order to be used in classification and regression. For an example, you provide a dataset with labeled pictures of cats and dogs. The algorithm takes the inputs and the labels, and learns to classify new pictures based upon the variety and known features of the trained dataset, and can classify new images based on that as either cat or dog.\n",
    "- Unsupervised Algorithms: Unsupervised algorithms use no labels to learn from data. Instead, it takes inputs and uses the feature vectors to try and find patterns in the dataset, and cluster or find a structure within that data. For example take a large dataset on the weather. This dataset may contain temperature, humidity, wind, etc as features, and lets say the model uses k-means clustering with 3 centroids. The model will try to find patterns in the data, and assign each data point with it's fature vector to a certain distance from a centroid. Data that is closest to a certain centroid is part of that cluster, and all the points that are nearest to that cluster would be somewhat similar based upon the patterns the algorithm finds.\n",
    "- Self-Supervised Algorithms: Self-Supervised algorithms are algorithms that take data from datasets with no labels, and creates labels from the data and uses those labels to perform supervised learning, without the need to provide already labeled data. An example of this would be BERT, where we predict missing words in a sentence by providing the sentence, using a dataset of words and masking the missing word, and allowing the algorith to create labels and run supervised learning on the data to predict the missing word based on the context of the sentence around the missing word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5). What are the differences between logistic regression and linear regression?**\n",
    "\n",
    "Linear regression is used to predict on continuous variables and predicts a continuous output. Logistic regression on the other hand, predicts the probability of something for classification of a categorical variable, rather than a continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6). Word2Vec produces the static feature vector. What does static feature vector or embedding mean? Explain static feature vector or embedding in the context of [apple fruit] [apple inc]**\n",
    "\n",
    "The static feature vector/embedding is a feature vector that does not change with context surrounding a word. Using the example Apple fruit and Apple inc in two sentences \"apple is a yummy fruit\" and \"apple inc is a robust company\", the word apple will be assigned the same feature vector across both sentences. No matter where apple appears, it will have the same feature vector(static) and therefore will lack context surrounding the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7). What do handcrafted and learned features mean? Why are learned features from neural networks, such as Word2Vec, preferred over handcrafted features like TF-IDF?**\n",
    "\n",
    "- Handcrafted Features: These are features that are designed by the engineers manually, based upon our knowledge of the dataset and what we are looking for.\n",
    "- Learned Features: These are features that are sutomatically discovered by the models we train, and are extracted from the dataset by these models in order to identify relationships and patterns.\n",
    "\n",
    "Learned features are preferred over handcrafted features because they can capture deeper abstract meanings wthin datasets that handcrafted features might miss because of preset constraints. The learned features also scale to data better, and captures meaning in datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8). Please define the evaluation metrics Sensitivity, Specificity, Precision, Recall, F1-score, Accuracy, and Matthew's Correlation Coefficient in the context of binary classification. Where are True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) located in the confusion matrix produced by scikit-learn module? Sometimes, accuracy can be misleading in such scenarios. What other metrics should be considered to assess the performance of a predictive model effectively?**\n",
    "\n",
    "- Sensitivity: Sensitivity is the ratio of True Positives compared to all the True Positives and False Negatives that the model catches. It means that there are low false negatives when sensitivity is high.\n",
    "- Specificity: Specificity is the ratio of True Negatives compared to the True Negatives and False Positives that the model catches. It means that there are low false positives when specificity is high.\n",
    "- Precision: Precision is the ratio of True Positives True Positives and False Poitives that the model catches. It means that there are few false positives when the precision is high.\n",
    "- Recall: Recall is the same as Sensitivity. Recall is the ratio of True Positives compared to all the True Positives and False Negatives that the model catches. It means that there are low false negatives when recall is high.\n",
    "- F1-Score: The F1-Score is the harmonic mean of Precision and Recall.\n",
    "- Accuracy: Accuracy is the ratio of how often the model is correct. The higher the accuracy, the more often the model is correct.\n",
    "- Matthew's Correlation Coefficient: The Matthews Correlation Coefficient is a metric used to measure prediction accuracy for models and is based off the Confusion Matrix. MCC of 1 is a perfect prediction, MCC of 0 is no better than random, and a MCC of 0 is a completely imperfect prediction.\n",
    "\n",
    "In the Confusion Matrix produced by scikit-learn module, TN is top left, FP is top right, FN is bottom left, and TP is bottom right.\n",
    "\n",
    "All metrics should be evaluated, however MCC and F1-Score can be utilized to assess the performance of a predictive model effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9). Can you explain where are the embeddings of the words encoded in the CBOW and Skip-gram Word2Vec algorithm? Explain with the relevant diagrams.**\n",
    "\n",
    "The embeddings of the words are encoded in the hidden layes in both CBOW and Skip-gram Word2Vec algorithm. \n",
    "\n",
    "![alt text](CBOW.png \"CBOW\") \n",
    "\n",
    "![alt text](Skip-Gram.png \"Skip-gram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10). What are the different types of techniques used to handle imbalance in the datasets? Explain in detail.**\n",
    "\n",
    "- Resampling Methods: Either duplicating more samples of the minority class or removing examples of the majority class can be used to balance classes in datasets.\n",
    "- Boosting: Uses many shallow decision trees to form stronger and stronger prediction models until we combine all the models outputs and finally predict from a weighted sum of all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11). What can you infer from the following two images or plots? Are they related? If they are related how are they related please explain.**\n",
    "\n",
    "Lower thresholds allow for more positives, while high thresholds allow for more negatives. The plot is a plot of False Positive Rate vs True Positive Rate, and the two images are related. They are related as the table shows confusion matrices for different thresholds, and it shows how the ROC curve is generated from the TP, FP, TN, and FN of different thresholds across the confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12). Word2Vec starts with random embedding vectors for words. After several training epochs on the dataset, these embeddings are refined. Could you explain what this process means and how the embeddings improve during training?**\n",
    "\n",
    "This process means that the words start with random vectors for each word, and as the model progresses through different training epochs, the vectors are refined using context of near words and the model learns relationships between words. The embeddings improve during training by each epoch there are small adjustments made to each word vector based upon near words and after many epochs the model can identify words that are near each other often and other relationships between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13). What do binary, multiclass, and multilabel classification mean? What types of loss functions are used for each of these classification methods?**\n",
    "\n",
    "- Binary Classification: A classification where there are only two possible classes. Loss function -> Binary Cross Entropy\n",
    "- Multiclass Classification: a classification where there are more than just two possible classes, and each input and output can only belong to one class. -> Categorical Cross Entropy\n",
    "- Multilabel Classification: a classification where there are more than just two possible classes, and each input can belong to more than just one class. -> Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14). What is the likely answer to the following question, and how do we arrive at the solution? King – Man + Woman = ?**\n",
    "\n",
    "The likely answer to the following question would be Queen, and we arrive at that solution through the Word2Vec embedding and subtracting and adding the actual vectors of the words, and the resulting vector would be a vector close to \"queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 15). Can you explain \"How to train custom word vectors Spring 2025 Final ipynb file.ipynb\" program in detail. (Available in the module section) Is it possible to change the vector dimension? How would you train using the Skip-gram or CBOW methods, and what parameters need to be adjusted for each? Additionally, what does it mean to freeze the embedding matrix?**\n",
    "\n",
    "The Program loads the data, then tokenizes the data and build a vocabulary based upon the data. It drops rows with more than 10 sentences per sample. It converts the caterogical labels to numerical classes as well. We do more cleaning by tokenizing the text, lwoercasing words, removing punctuation and stopwords. The program then trains custom Word2Vec Embeddings, using 100 dimensional embeddings, considering a window of 5 words, and using skip-gram. It then creates an embedding matrix and fills it with vectors from the Word2Vec embeddings. It then uses this to integrate into a Neural Network using Embedding layer and the custom vectors. \n",
    "\n",
    "It is possible to change the vector dimension, by changing the vector_size parameter.\n",
    "\n",
    "We train using Skip-gram by setting sg = 1, and CBOW by setting sq = 0.\n",
    "\n",
    "Freezing the embedding matrix stops the model from updating the embedding matrix during it's training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 16). What is a Receiver Operating Characteristic (ROC) curve, and what does it represent?**\n",
    "\n",
    "ROC Curve is a plot that is used to evaluate the performance of a Binary Classification Model across many different thresholds. It represents the model's ability to tell the difference between positive and negative classes, and is the relationship between True Positive Rate and False Positive Rate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
